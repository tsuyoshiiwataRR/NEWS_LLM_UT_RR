{
    "thread": {
        "uuid": "9c3e3b4de48776912497e4a4dd7096a03c8e66b5",
        "url": "https://discuss.huggingface.co/t/meta-llama-llama-3-2-90b-vision-instruct-continually-crashing-with-torch-outofmemoryerror-cuda-out-of-memory-tried-to-allocate/126688",
        "site_full": "discuss.huggingface.co",
        "site": "huggingface.co",
        "site_section": "https://discuss.huggingface.co/posts.rss",
        "site_categories": [],
        "section_title": "Hugging Face Forums - Latest posts",
        "title": "\"meta-llama/Llama-3.2-90B-Vision-Instruct\" continually crashing with \"torch.OutOfMemoryError: CUDA out of memory. Tried to allocate\"",
        "title_full": "\"meta-llama/Llama-3.2-90B-Vision-Instruct\" continually crashing with \"torch.OutOfMemoryError: CUDA out of memory. Tried to allocate\"",
        "published": "2024-11-22T05:09:00.000+02:00",
        "replies_count": 0,
        "participants_count": 1,
        "site_type": "news",
        "country": "CO",
        "main_image": "https://global.discourse-cdn.com/hellohellohello/original/2X/d/de4155eb4aa4108ecb32a1389d7cc37ae69f88b7.png",
        "performance_score": 0,
        "domain_rank": 1386,
        "domain_rank_updated": "2024-11-18T23:00:00.000+02:00",
        "social": {
            "facebook": {
                "likes": 0,
                "comments": 0,
                "shares": 0
            },
            "vk": {
                "shares": 0
            }
        }
    },
    "uuid": "9c3e3b4de48776912497e4a4dd7096a03c8e66b5",
    "url": "https://discuss.huggingface.co/t/meta-llama-llama-3-2-90b-vision-instruct-continually-crashing-with-torch-outofmemoryerror-cuda-out-of-memory-tried-to-allocate/126688",
    "ord_in_thread": 0,
    "parent_url": null,
    "author": "@twelcome Traiano Welcome",
    "published": "2024-11-22T05:09:00.000+02:00",
    "title": "\"meta-llama/Llama-3.2-90B-Vision-Instruct\" continually crashing with \"torch.OutOfMemoryError: CUDA out of memory. Tried to allocate\"",
    "text": "I’m trying to run “meta-llama/Llama-3.2-90B-Vision-Instruct” as a containerized vllm on openshift with a single NVIDIA v100 GPU (with 8 GPUs available).\nThe pod runs, however after about 2 minutes fails with a large error trace which includes the following error:\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 80.31 MiB is free. Process 2718503 has 42.07 GiB memory in use. Process 3312520 has 36.93 GiB memory in use. Of the allocated memory 33.80 GiB is allocated by PyTorch, and 55.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation. See documentation for Memory Management https://pytorch.org/docs/stable/notes/cuda.html#environment-variables [rank0]:[W1121 09:26:13.404528307 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\nThis suggests a shortage of memory resources for the GPU, but the GPU appears to be under-utilised and the server doesn’t seem to be heavily utilised.\n+-----------------------------------------------------------------------------------------+ | NVIDIA-SMI 550.127.05 Driver Version: 550.127.05 CUDA Version: 12.4 | |-----------------------------------------+------------------------+----------------------+ | GPU Name Persistence-M | Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap | Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |=========================================+========================+======================| | 0 NVIDIA H100 80GB HBM3 On | 00000000:DB:00.0 Off | 0 | | N/A 27C P0 69W / 700W | 4MiB / 81559MiB | 0% Default | | | | Disabled | +-----------------------------------------+------------------------+----------------------+ +-----------------------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=========================================================================================| | No running processes found | +-----------------------------------------------------------------------------------------+\nThis leads me to wonder if the problem is with the configuration of the pod, including the arguments passed to the LLM.\ncontainer: name: pepper-vllm-90b image: pepperairepo.azurecr.io/vllm/vllm-openai:0.6.3 imagePullPolicy: IfNotPresent args: - --model - meta-llama/Llama-3.2-90B-Vision-Instruct - --gpu-memory-utilization - \"0.6\" - --max_model_len - \"4096\" - --max_num_seqs - \"4\" - --enforce_eager - --tensor-parallel-size - \"4\" volumeMounts: cacheVolume: /root/.cache/huggingface # \"/Model2\" <- path from nfs where the model is stored containerPort: 8000 shmSize: 16Gi\nI’ve varied these parameters according to many suggestions on the internet, including changing --tensor-parallel-size. Nothing seems to stop the memory related crash.\n--tensor-parallel-size\nQuestions:\na) Has anyone encountered this problem before? b) Any other troubleshooting approaches for diagnosing and fixing this issue?\nFor completeness, here is the full context around the memory failure:\nINFO 11-21 09:26:12 selector.py:115] Using XFormers backend. ESC[1;36m(VllmWorkerProcess pid=352)ESC[0;0m INFO 11-21 09:26:12 selector.py:115] Using XFormers backend. ESC[1;36m(VllmWorkerProcess pid=353)ESC[0;0m INFO 11-21 09:26:12 multiproc_worker_utils.py:242] Worker exiting ESC[1;36m(VllmWorkerProcess pid=351)ESC[0;0m INFO 11-21 09:26:12 multiproc_worker_utils.py:242] Worker exiting ESC[1;36m(VllmWorkerProcess pid=352)ESC[0;0m INFO 11-21 09:26:12 multiproc_worker_utils.py:242] Worker exiting INFO 11-21 09:26:12 multiproc_worker_utils.py:121] Killing local vLLM worker processes Process SpawnProcess-1: Traceback (most recent call last): File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap self.run() File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run self._target(*self._args, **self._kwargs) File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 392, in run_mp_engine engine = MQLLMEngine.from_engine_args(engine_args=engine_args, ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 141, in from_engine_args return cls( ^^^^ File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/multiprocessing/engine.py\", line 78, in __init__ self.engine = LLMEngine(*args, ^^^^^^^^^^^^^^^^ File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 335, in __init__ self.model_executor = executor_class( ^^^^^^^^^^^^^^^ File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 26, in __init__ super().__init__(*args, **kwargs) File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 47, in __init__ self._init_executor() File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 111, in _init_executor : self._run_workers(\"load_model\", File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 192, in _run_workers driver_worker_output = driver_worker_method(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 183, in load_model self.model_runner.load_model() File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1062, in load_model self.model = get_model(model_config=self.model_config, ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 19, in get_model return loader.load_model(model_config=model_config, ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 398, in load_model model = _initialize_model(model_config, self.load_config, ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 175, in _initialize_model return build_model( ^^^^^^^^^^^^ File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/loader.py\", line 160, in build_model return model_class(config=hf_config, ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/mllama.py\", line 912, in __init__ self.language_model = MllamaForCausalLM( ^^^^^^^^^^^^^^^^^^ File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/mllama.py\", line 856, in __init__ self.model = MllamaTextModel(config, cache_config, quant_config) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/mllama.py\", line 794, in __init__ LlamaDecoderLayer(config, File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 229, in __init__ self.mlp = LlamaMLP( ^^^^^^^^^ File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/llama.py\", line 71, in __init__ self.gate_up_proj = MergedColumnParallelLinear( ^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py\", line 424, in __init__ super().__init__(input_size=input_size, File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py\", line 304, in __init__ self.quant_method.create_weights( File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py\", line 122, in create_weights weight = Parameter(torch.empty(sum(output_partition_sizes), ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_device.py\", line 79, in __torch_function__ return func(*args, **kwargs) ^^^^^^^^^^^^^^^^^^^^^ torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 79.10 GiB of which 80.31 MiB is free. Process 2718503 has 42.07 GiB memory in use. Process 3312520 has 36.93 GiB memory in use. Of the allocated memory 33.80 GiB is allocated by PyTorch, and 55.48 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation. See documentation for Memory Management https://pytorch.org/docs/stable/notes/cuda.html#environment-variables [rank0]:[W1121 09:26:13.404528307 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors] Traceback (most recent call last): File \"<frozen runpy>\", line 198, in _run_module_as_main File \"<frozen runpy>\", line 88, in _run_code File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 585, in <module> uvloop.run(run_server(args)) File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 109, in run return __asyncio.run( ^^^^^^^^^^^^^^ File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run return runner.run(main) return __asyncio.run( ^^^^^^^^^^^^^^ File \"/usr/lib/python3.12/asyncio/runners.py\", line 194, in run return runner.run(main) ^^^^^^^^^^^^^^^^ File \"/usr/lib/python3.12/asyncio/runners.py\", line 118, in run return self._loop.run_until_complete(task) ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"uvloop/loop.pyx\", line 1517, in uvloop.loop.Loop.run_until_complete File \"/usr/local/lib/python3.12/dist-packages/uvloop/__init__.py\", line 61, in wrapper return await main ^^^^^^^^^^ File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 552, in run_server async with build_async_engine_client(args) as engine_client: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__ return await anext(self.gen) ^^^^^^^^^^^^^^^^^^^^^ File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 107, in build_async_engine_client async with build_async_engine_client_from_engine_args( ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ File \"/usr/lib/python3.12/contextlib.py\", line 210, in __aenter__ return await anext(self.gen) ^^^^^^^^^^^^^^^^^^^^^ File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/api_server.py\", line 194, in build_async_engine_client_from_engine_args raise RuntimeError( RuntimeError: Engine process failed to start /usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown warnings.warn('resource_tracker: There appear to be %d '\nSome information on the physical server running this vllm:\noc describe node xxxx\nAllocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- ---",
    "highlightText": "",
    "highlightTitle": "",
    "highlightThreadTitle": "",
    "language": "english",
    "sentiment": "negative",
    "categories": [
        "Science and Technology"
    ],
    "ai_allow": true,
    "canonical": false,
    "webz_reporter": false,
    "external_links": [],
    "external_images": [],
    "entities": {
        "persons": [],
        "organizations": [],
        "locations": []
    },
    "syndication": {
        "syndicated": false,
        "syndicate_id": null,
        "first_syndicated": false
    },
    "rating": null,
    "crawled": "2024-11-22T05:13:10.062+02:00",
    "updated": "2024-11-22T05:13:10.062+02:00"
}