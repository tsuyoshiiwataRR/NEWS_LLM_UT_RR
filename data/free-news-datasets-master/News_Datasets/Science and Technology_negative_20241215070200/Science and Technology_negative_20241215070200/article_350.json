{
    "thread": {
        "uuid": "b4858ee885daae99a5ba4320fefb9be712e17f3b",
        "url": "https://news.ycombinator.com/item?id=42125888",
        "site_full": "news.ycombinator.com",
        "site": "ycombinator.com",
        "site_section": "http://b.hatena.ne.jp/hotentry/it.rss",
        "site_categories": [
            "society",
            "entertainment"
        ],
        "section_title": "&#x306F;&#x3066;&#x306A;&#x30D6;&#x30C3;&#x30AF;&#x30DE;&#x30FC;&#x30AF; - &#x4EBA;&#x6C17;&#x30A8;&#x30F3;&#x30C8;&#x30EA;&#x30FC; - &#x30C6;&#x30AF;&#x30CE;&#x30ED;&#x30B8;&#x30FC;",
        "title": "OpenAI, Google and Anthropic are struggling to build more advanced AI | Hacker News",
        "title_full": "OpenAI, Google and Anthropic are struggling to build more advanced AI | Hacker News",
        "published": "2024-11-15T05:12:00.000+02:00",
        "replies_count": 0,
        "participants_count": 0,
        "site_type": "news",
        "country": "US",
        "main_image": "",
        "performance_score": 0,
        "domain_rank": 879,
        "domain_rank_updated": "2024-11-11T23:00:00.000+02:00",
        "social": {
            "facebook": {
                "likes": 0,
                "comments": 0,
                "shares": 0
            },
            "vk": {
                "shares": 0
            }
        }
    },
    "uuid": "b4858ee885daae99a5ba4320fefb9be712e17f3b",
    "url": "https://news.ycombinator.com/item?id=42125888",
    "ord_in_thread": 0,
    "parent_url": null,
    "author": null,
    "published": "2024-11-15T05:12:00.000+02:00",
    "title": "OpenAI, Google and Anthropic are struggling to build more advanced AI | Hacker News",
    "text": "I lead a team exploring cutting edge LLM applications and end-user features. It's my intuition from experience that we have a LONG way to go.\nGPT-4o / Claude 3.5 are the go-to models for my team. Every combination of technical investment + LLMs yields a new list of potential applications.\nFor example, combining a human-moderated knowledge graph with an LLM with RAG allows you to build \"expert bots\" that understand your business context / your codebase / your specific processes and act almost human-like similar to a coworker in your team.\nIf you now give it some predictive / simulation capability - eg: simulate the execution of a task or project like creating a github PR code change, and test against an expert bot above for code review, you can have LLMs create reasonable code changes, with automatic review / iteration etc.\nSimilarly there are many more capabilities that you can ladder on and expose into LLMs to give you increasingly productive outputs from them.\nChasing after model improvements and \"GPT-5 will be PHD-level\" is moot imo. When did you hire a PHD coworker and they were productive on day-0 ? You need to onboard them with human expertise, and then give them execution space / long-term memories etc to be productive.\nModel vendors might struggle to build something more intelligent. But my point is that we already have so much intelligence and we don't know what to do with that. There is a LOT you can do with high-schooler level intelligence at super-human scale.\nTake a naive example. 200k context windows are now available. Most people, through ChatGPT, type out maybe 1500 tokens. That's a huge amount of untapped capacity. No human is going to type out 200k of context. Hence why we need RAG, and additional forms of input (eg: simulation outcomes) to fully leverage that.\nreply\nYes there seems to be lots of potential. Yes we can brainstorm things that should work. Yes there is a lot of examples of incredible things in isolation. But it's a little bit like those youtube videos showing amazing basketball shots in 1 try, when in reality lots of failed attempts happened beforehand. Except our users experience the failed attempts (LLM replies that are wrong, even when backed by RAG) and it's incredibly hard to hide those from them.\nShow me the things you / your team has actually built that has decent retention and metrics concretely proving efficiency improvements.\nLLMs are so hit and miss from query to query that if your users don't have a sixth sense for a miss vs a hit, there may not be any efficiency improvement. It's a really hard problem with LLM based tools.\nThere is so much hype right now and people showing cherry picked examples.\nThis has been my team's experience (and frustration) as well, and has led us to look at using LLMs for classifying / structuring, but not entrusting an LLM with making a decision based on things like a database schema or business logic.\nI think the technology and tooling will get there, but the enormous amount of effort spent trying to get the system to \"do the right thing\" and the nondeterministic nature have really put us into a camp of \"let's only allow the LLM to do things we know it is rock-solid at.\"\nEven this is insanely hard in my opinion. The one thing that you would assume LLM to excel at is spelling and grammar checking for the English language, but even the top model (GPT-4o) can be insanely stupid/unpredictable at times. Take the following example from my tool:\nhttps://app.gitsense.com/?doc=6c9bada92&model=GPT-4o&samples...\n5 models are asked if the sentence is correct and GPT-4o got it wrong all 5 times. It keeps complaining that GitHub is spelled like Github, when it isn't. Note, only 2 weeks ago, Claude 3.5 Sonnet did the same thing.\nI do believe LLM is a game changer, but I'm not convinced it is designed to be public-facing. I see LLM as a power tool for domain experts, and you have to assume whatever it spits out may be wrong, and your process should allow for it.\nEdit:\nI should add that I'm convinced that not one single model will rule them all. I believe there will be 4 or 5 models that everybody will use and each will be used to challenge one another for accuracy and confidence.\nWhile LLMs do plenty of awful things, people make the most incredibly stupid mistakes too, and that is what LLMs needs to be benchmarked against. The problem is that most of the people evaluating LLMs are better educated than most and often smarter than most. When you see any quantity of prompts input by a representative sample of LLM losers, you quickly lose all faith in humanity.\nI'm not saying LLMs are good enough. They're not. But we will increasingly find that there are large niches where LLMs are horrible and error prone yet still outperform the people companies are prepared to pay to do the task.\nIn other words, on one hand you'll have domain experts becoming expert LLM-wranglers. On the other hand you'll have public-facing LLMs eating away at tasks done by low paid labour where people can work around their stupid mistakes with process or just accepting the risk, same as they currently do with undertrained labor.\nThis means that on one hand firms are demanding RTO for culture and team work improvements. While on the other they will be ok with a tool that makes unpredictable errors like humans, but can never be impacted by culture and team work.\nThese two ideas lie in odd juxtaposition to each other.\nI am 100% not blaming the LLM, but rather VCs and the media for believing the VCs. Once we get over the hype and people realize there isn't a golden goose, the better off we will be. Once we accept that LLM is not perfect and that it is not what we are being sold, I believe we will find a place for it that will make a huge impact. Unfortunately for OpenAI and others, I don't believe they will play as big of a role as they would like us to believe/will.\nthis gets to the heart of it for me. I think LLMs are an incredible tool, providing advanced augmentation on our already developed search capabilities. What advanced user doesnt want to have a colleague they can talk about their specific domain capacity with?\nThe problem comes from the hyperscaling ambitions of the players who were the first in this space. They quickly hyped up the technology beyond want it should have been.\n- every time a different result is produced.\n- no reasoning capabilities were categorically determined.\nSo this is it. If you want LLM - brace for different results and if this is okay for your application (say it’s about speech or non-critical commands) then off you are.\nOtherwise simply forget this approach, and particularly when you need reproducible discreet results.\nI don’t think it gets any better than that and nothing so far implicated it will (with this particular approach to AGI or whatever the wet dream is)\nThere’s a whole classification of tasks where a human can look at a body of work and determine whether it’s correct or not in far less time than it would take for them to produce the work directly.\nAs a random example, having LLMs write unit tests.\nWhich Apple engineers? Yours is the only reference to the company in this comment section or in the article.\nI have had good luck using an LLM as a \"sanity checking\" layer for transcription output, though. A simple prompt like \"is this paragraph coherent\" has proven to be a pretty decent way to check the accuracy of whisper transcriptions.\nhttps://app.gitsense.com/?doc=905f4a9af74c25f&model=Claude+3...\nClaude 3.5 Sonnet will now misinterpret \"GitHub as \"Github\"\nI feel like this is unfair. That's the only thing it got wrong? But we want it to pass all of our evals, even ones the perhaps a dictionary would be better at solving? Or even an LLM augmented with a dictionary.\nLLM has its place and it will forever change how we think about UX and other things, but we need to realize you really can't create a public facing solution without significant safe guards, if you don't want egg on your face.\nAs a user I want it to be right, even if that contradicts the normal rules of the language.\nLeaving aside \"we're\" and \"we are\" are the same, it is absolutely active voice\nIn education at least, we've actively improved efficiency by ~25% across a large swath of educators (direct time saved) - agentic evaluators, tutors and doubt clarifiers. The wins in this industry are clear. And this is that much more time to spend with students.\nI also know from 1-1 conversation with my peers in large-finance world, and there too the efficiency improvements on multiple fronts are similar.\nI see these statements often here about “I’ve never seen an effective commercial use of LLMs,” which tells me you aren’t working with very creative and competent people in areas that are amenable to LLMs. In my professional network beyond where I work now I know at least a dozen people who have successful commercial applications of LLMs. They tend to be highly capable people able to build the end to end tool chains necessary (which is a huge gap) and understand how to compose LLMs in hierarchical agents with effective guard rails. Most ineffectual users of LLMs want them to be lazy buttons that obviate the need to think. They’re not - like any sufficiently powerful tool they require thought up front and are easy to use wrong. This will get better with time as patterns and tools emerge to get the most use out of them in a commercial setting. However the ability to process natural language and use an emergent (if not actual) abductive reasoning is absurdly powerful and was not practically possible 4 years ago - the assertion such an amazing capability in an information or decisioning system is not commercially practical is on the face absurd.\nApps that use LLMs or apps made with LLMs? In either case can you share them?\n>which tells me you aren’t working with very creative and competent people\n> In my professional network beyond where I work now I know at least a dozen people who have successful commercial applications of LLMs.\nNo one doubts that you can integrate LLMs into an application workflow and get some benefits in certain cases. That has been what the excitement and promise was about all along. They have a demonstrated ability to wrangle, extract, and transform data (mostly correctly) and generate patterns from data and prompts (hit and miss, usually with a lot of human involvement). All of which can be powerful. But outside of textual or visual chatbots or CRUD apps, no one wants to \"put up or shut\" a solid example that the top management of an existing company would sign off on. Only stories about awesome examples they and their friends are working on ... which often turn out to be CRUD apps or textual or visual chatbots. One notable standout is generative image apps can be quite good in certain circumstances.\nSo, since you seem to have a real interest and actual examples of this, I am curious to see some that real companies would gamble that company on. And I don't mean some quixotic startup, I mean a company making real money now with customers that is confident on that app to the point they are willing to risk big. Because that last part is what companies do with other (non LLM) apps. I also know that people aren't perfect and wouldn't expect an LLM to be, just want to make sure I am not missing something.\nCould you elaborate? Is this related to the \"teams of specialized LLMs\" concept I saw last year when Auto-GPT was getting a lot of hype?\nat the end of the day though, it's not exactly reliable or particularly transformative when you get past the party tricks\nThe theory behind these models so aggressively lags the engineering that I suspect there are many major improvements to be found just by understanding a bit more about what these models are really doing and making re-designs based on that.\nI highly encourage anyone seriously interested in LLMs to start spending more time in the open model space where you can really take a look inside and play around with the internals. Even if you don't have the resources for model training, I feel personally understanding sampling and other potential tweaks to the model (lots of neat work on uncertainty estimations, manipulating the initial embedding the prompts are assigned, intelligent backtracking, etc).\nAnd from a practical side I've started to realize that many people have been holding on of building things waiting for \"that next big update\", but there a so many small, annoying tasks that can be easily automated.\n> You can have it craft an email, or to review your email, but I wouldn't trust an LLM with anything mission-critical\nMy point is that an entire world lies between these two extremes.\nI mostly use AIs in writing as a glorified grammar checker that sometimes suggests alternate phrasing. I do the initial writing and send it to an AI for review. If I like the suggestions I may incorporate some. Others I ignore.\nThe only times I use it to write is when I have something like a status report and I’m having a hard time phrasing things. Then I may write a series of bullet points and send that through an AI to flesh it out. Again, that is just the first stage and I take that and do editing to get what I want.\nIt’s just a tool, not a creator.\nI believe the above suggested that this type of email likely doesn't need to be sent. Is anyone really reading the status report? If they read it, what concrete decisions do they make based on it. We all get in this trap of doing what people ask of us but it often isn't what shareholders and customers really care about.\nI’ve noticed this too — I’ve been calling it intellectual deflation. By analogy, why spend now when it may be cheaper in a month? Why do the work now, when it will be easier in a month?\nFor LLMs, we don't even know how to reliably measure performance, much less plan for expected improvements.\nDoesn’t need to be comprehensive, I just don’t know where to jump off from.\nThis ofc implies local models and that you have a decent cpu + min 64gb of ram to run above 7b-sized model.\nhttps://github.com/oobabooga/text-generation-webui\nhttps://huggingface.co/models?pipeline_tag=text-generation&s...\nAlso we only hear / see the examples that are meant to scale. Startups typically offer up something transformative, ready to soak up a segment of a market. And that’s hard with the current state of LLMs. When you try their offerings, it’s underwhelming. But there is richer, more nuanced hard to reach fruits that are extremely interesting - but it’s not clear where they’d scale in and of themselves.\nThe problem is that 99% of theories are hard to scale.\nI am not an expert, as I work adjacent to this field, but I see the inverse - dumbing down theory to increase parallelism/scalability.\nIt works disturbingly well. But because it doesn’t have any actual intrinsic knowledge it has no way of knowing when it made a “good“ hallucination versus a “bad“ one.\nI’m sure people are working at piling things on top to try and influence what gets generated or catch and move away from errors errors other layers spot… but how much effort and resources will be needed to make it “good enough“ that people don’t worry about this anymore.\nIn my mind the core problem is people are trying to use these for things they’re unsuitable for. Asking fact-based questions is asking for trouble. There isn’t much of a wrong answer if you wanted to generate a bedtime story or a bunch of test data that looks sort of like an example you give it.\nIf you ask it to find law cases on a specific point you’re going to raise a judge‘s ire, as many have already found.\nThe scaling laws may be dead. Does this mean the end of LLM advances? Absolutely not.\nThere are many different ways to improve LLM capabilities. Everyone was mostly focused on the scaling laws because that worked extremely well (actually surprising most of the researchers).\nBut if you're keeping an eye on the scientific papers coming out about AI, you've seen the astounding amount of research going on with some very good results, that'll probably take at least several months to trickle down to production systems. Thousands of extremely bright people in AI labs all across the world are working on finding the next trick that boosts AI.\nOne random example is test-time compute: just give the AI more time to think. This is basically what O1 does. A recent research paper suggests using it is roughly equivalent to an order of magnitude more parameters, performance wise. (source for the curious: https://lnkd.in/duDST65P)\nAnother example that sounds bonkers but apparently works is quantization: reducing the precision of each parameter to 1.58 bits (ie only using values -1, 0, 1). This uses 10x less space for the same parameter count (compared to standard 16-bit format), and since AI operatons are actually memory limited, directly corresponds to 10x decrease in costs: https://lnkd.in/ddvuzaYp\n(Quite apart from improvements like these, we shouldn't forget that not all AIs are LLMs. There's been tremendous advance in AI systems for image, audio and video generation, interpretation and munipulation and they also don't show signs of stopping, and there's possibility that a new or hybrid architecture for the textual AI might be developed).\nAI winter is a long way off.\n- Jim Keller\nhttps://www.youtube.com/live/oIG9ztQw2Gc?si=oaK2zjSBxq2N-zj1...\nBut Goodhart's law; \"When a measure becomes a target, it ceases to be a good measure\"\nDirectly applies here, Moore's Law was used to set long term plans at semiconductor companies, and Moore didn't have empirical evidence it was even going to continue.\nIf you say, arbitrarily decide CPU, or worse, single core performance as your measurement, it hasn't held for well over a decade.\nIf you hold minimum feature size without regard to cost, it is still holding.\nWhat you want to prove usually dictates what interpretation you make.\nThat said, the scaling law is still unknown, but you can game it as much as you want in similar ways.\nGPT4 was already hinting at an asymptote on MMLU, but the question is if it is valid for real work etc...\nTime will tell, but I am seeing far less optimism from my sources, but that is just anecdotal.\nAlso because it was easy, and expense was not the first concern.\nThe > 100 P/E ratios we are already seeing can't be justified by something as quotidian as the exceptionally good productivity tools you're talking about.\nWhat are you basing this on?\nIT outsourcing is a $500+ billion industry. If OpenAI et al can run even a 10% margin, that business alone justifies their valuation.\nNobody knows how things like coding assistants or other AI applications will pan out. Maybe it'll be Oracle selling Meta-licenced solutions that gets the lion's share of the market. Maybe custom coding goes away for many business applications as off-the-shelf solutions get smarter.\nA future where all that AI (or some hypothetical AGI) changes is work being done by humans to the same work being done by machines seems way too linear.\nThe big one being I'm not assuming AGI. Low-level coding tasks, the kind frequently outsourced, are within the realm of being competitive with offshoring with known methods. My point is we don't need to assume AGI for these valuations to make sense.\nIf there is one domain where we're seeing tangible progress from AI, it's in working towards this goal. Difficult projects aren't in scope. But most tech, especially most tech branded IT, is not difficult. Everyone doesn't need an inventory or customer-complaint system designed from scratch. Current AI is good at cutting through that cruft.\nLLMs are in my opinion hamstrung at the starting gate in regards to replacing software teams, as they would need to be able to understand complex business requirements perfectly, which we know they cannot. Humans can't either. It takes a business requirements/integration logic/code generation pipeline and I think the industry is focused on code generation and not that integration step.\nI think there needs to be a re-imaging of how software is built by and for interaction with AI if it were to ever take over from human software teams, rather than trying to get AI to reflect what humans do.\nAre they good enough to replace a human yet? Questionable[0], but they are improving.\n[0] You wouldn't believe how low the outsourcing contractors' quality can go. Easily surpassed by current AI systems :) That's a very low bar tho.\nOnce massively useful AI has been achieved, or it's been determined that LLMs are it, then it becomes a race to the bottom as GOOG/MSFT/AMZN/META/etc design/deploy more specialized accelerators to deliver this final form solution as cheaply as possible.\nMost other businesses trying to actually use LLMs are the riskier ones, including OpenAI, IMO (though OpenAI is perhaps the least risky due to brand recognition).\nI find that a human is able to solve a P=NP situation, and an LLM can’t quite yet do that. When they can the game changes.\nI mean, it's pretty clear to me they're a potentially great human-machine interface, but trying to make LLMs - in their current fundamental form - a reliable computational tool.. well, at best it's an expensive hack, but it's just not the right tool for the job.\nI expect the next leap forward will require some orthogonal discovery and lead to a different kind of tool. But perhaps we'll continue to use LLMs as we knownthem now for what they're good at - language.\nIt's possible, but it's not at all obvious and requires a slightly skewed way of looking at them.\nIt's been a while though, we've had great models now for a 18 months plus. Why are we still yet to see these type of applications rolling out on a wide scale?\nMy anecdotal experience is that almost universally, 90-95% type accuracy you get from them is just not good enough. Which is to say, having something be wrong 10% or even 5% of the time is worse than not having at all. At best, you need to implement applications like that in an entirely new paradigm that is designed to extract value without bearing the costs of the risks.\nIt doesn't mean LLMs can't be useful, but they are kind of stuck with applications that inherently mesh with human oversight (like programming etc). And the thing about those is that they don't really scale, because the human oversight has to scale up with whatever the LLM is doing.\nI.e. can it ruminate on the data it's ingested, and rather than returning the response of highest probability, return something original?\nI think that's the key. If LLMs can't ultimately do that, there's still a lot to be gained from utilising the speed and fluidly scalable resources of computers.\nBut like all the top tech companies know, it's not quantity of bodies in seats that matters but talent, the thing that's going to prevail is raw intelligence. If it can't think better than us, just process data faster and more voluminously but still needing human verification, we're on an asymptotic path.\nAnd while Qwen2.5-Coder-32B-Instruct is a pretty advanced finetune — it was trained on an extra 5 trillion tokens — even smaller finetunes have done really well. For example, Dracarys-72B, which was a simpler finetune of Qwen2.5-72B using a modified version of DPO on a handmade set of answers to GSM8K, ARC, and HellaSwag, significantly outperforms the base Qwen2.5-72B model on the aider coding benchmarks.\nThere's a lot of intelligence we're leaving on the floor, because everyone is just prompting generic chat-tuned models! If you tune it to do something else, it'll be really good at the something else.\nNobody who takes code health and sustainability seriously wants to hear this. You absolutely do not want to be in a position where something breaks, but your last 50 commits were all written and reviewed by an LLM. Now you have to go back and review them all with human eyes just to get a handle on how things broke, while customers suffer. At this scale, it's an effort multiplier, not an effort reducer.\nIt's still good for generating little bits of boilerplate, though.\nCertainly not.\nBut technology is all about stacks. Each layer strives to improve, right up through UX and business value. The uses for 1µm chips had not been exhausted in 1989 when the 486 shipped in 800nm. 250nm still had tons of unexplored uses when the Pentium 4 shipped on 90nm.\nTalking about scaling at the the model level is like talking about transistor density for silicon: it's interesting, and relevant, and we should care... but it is not the sole determinent of what use cases can be build and what user value there is.\nIMO we've not even exhausted the options for spreadsheets, let alone LLMs.\nAnd the reason I'm thinking of spreadsheets is that they, like LLMs, are very hard to win big on even despite the value they bring. Not \"no moat\" (that gets parroted stochastically in threads like these), but the moat is elsewhere.\nIs there an AI tool that can ingest a codebase and locate code based on abstract questions? Like: \"I need to invalidate customers who haven't logged in for a month\" and it can locate things like relevant DB tables, controllers, services, etc.\nI tried building a whole codebase inspector, essentially what you are referring to with Gemini's 2 million token context window but had troubles with their API when the payload got large. Just 500 error with no additional info so...\nI don't know how many team meetings PhD students have, but I do know about software development jobs with 15 minute daily standups, and that length meeting at 120 words per minute for 5 days a week, 48 weeks per year of a 3 year PhD is 1.296.000 words.\nThat means employees who use LLM are, on average, recognizably bad. Those who are good enough, are also good enough to write the code manually.\nTo the point I wonder whether this HN thread is generated by OpenAI, trying to create buzz around AI.\n2. I'm not commenting on the quality, because they were writing about something that doesn't exist and therefore that's clearly just a given for the discussion. The only thing I was adding is that humans also need guidance, and quite a lot of it — even just a two-week sprint's worth of 15 minute daily stand-up meetings is 18,000 words, which is well beyond the point where I'd have given up prompting an LLM and done the thing myself.\nYes, existing LLMs are useful. Yes, there are many more things we can do with this tech.\nHowever, existing SOTA models are large, expensive to run, still hallucinate, fail simple logic tests, fail to do things a poorly trained human can do on autopilot, etc.\nThe performance of LLMs is extremely variable, and it is hard to anticipate failure.\nMany potential applications of this technology will not tolerate this level of uncertainty. Worse solutions with predictable and well understood shortcomings will dominate.\nThe meaning here is different. What I'm saying is that big companies like OpenAI will always strive to make a generic AI, such that anyone can do basically anything using AI. The big companies therefore will indeed (like you say) have a profitable business, but few others will.\nhttps://www.biorxiv.org/content/10.1101/2024.07.01.600583v1\nIf indeed the \"GPT 5!\" Arms race has calmed down, it should help everyone focus on the possible, their own goals, and thus what AI capabilities to deploy.\nJust as there won't be a \"Silver Bullet\" next gen model, the point about Correct Data In is also crucial. Nothing is 'free' not even if you pay a vendor or integrator. You, the decision making organization, must dedicate focus to putting data into your new AI systems or not.\nIt will look like the dawn of original IBM, and mechanical data tabulation, in retrospect once we learn how to leverage this pattern to its full potential.\nAs a developer, I'm making much more progress using the SOTA (Claude 3.5) as a Socratic interrogator. I'm brainstorming a project, give it my current thoughts, and then ask it to prompt me with good follow-up questions and turn general ideas into a specific, detailed project plan, next steps, open questions, and work log template. Huge productivity boost, but definitely not replacing me as an engineer. I specifically prompt it to not give me solutions, but rather, to just ask good questions.\nI've also used Claude 3.5 as (more or less) a free arbitrator. Last week, I was in a disagreement with a colleague, who was clearly being disingenuous by offering to do something she later reneged on, and evading questions about follow up. Rather than deal with organizational politics, I sent the transcript to Claude for an unbiased evaluation, and it \"objectively\" confirmed what had been frustrating me. I think there's a huge opportunity here to use these things to detect and call out obviously antisocial behavior in organizations (my CEO is intrigued, we'll see where it goes). Similarly, in our legal system, as an ultra-low-cost arbitrator or judge for minor disputes (that could of course be appealed to human judges). Seems like the level of reasoning in Claude 3.5 is good enough for that.\nMy mental model is always \"low-risk search\". https://muldoon.cloud/2023/10/29/ai-commandments.html\nHowever, this is better thought of as \"business logic scripting/automation\", not the magic employee-replacing AGI that would be the revolution some people are expecting. Maybe you can now build a slightly less shitty automated telephone response system to piss your customers off with.\nWhat gets pushed out isn’t the last version of the document itself (since it’s FIFO), but the important parts of the conversation—things like the rationale, requirements, or any context the model needs to understand why it’s making changes. So, instead of being helpful, that extra capacity just gets filled with old, repetitive chunks that have to be processed every time, muddying up the output. This isn’t just an issue with code; it happens with any kind of document editing where you’re going back and forth, trying to refine the result.\nSometimes I feel the way to \"resolve\" this is to instead go back and edit some earlier portion of the chat to update it with the \"new requirements\" that I didn't even know I had until I walked down some rabbit hole. What I end up with is almost like a threaded conversation with the LLM. Like, I sometimes wish these LLM chatbots explicitly treated the conversion as if it were threaded. They do support basically my use case by letting you toggle between different edits to your prompts, but it is pretty limited and you cannot go back and edit things if you do some operations (eg: attach a file).\nSpeaking of context, it's also hard to know what things like ChatGPT add to it's context in the first place. Many of times I'll attach a file or something and discover it didn't \"read\" the file into it's context. Or I'll watch it fire up a python program it writes that does nothing but echo the file into it's context.\nI think there is still a lot of untapped potential in strategically manipulating what gets placed into the context window at all. For example only present the LLM with the latest and greatest of a document and not all the previous revisions in the thread.\nMore realistically it’s like a really great sidekick for doing very specific mundane but otherwise non deterministic tasks.\nI think we’ll start to see AI permeate into nearly every back office job out there, but as a series of tools that help the human work faster. Not as one big brain that replaces the human.\nAction oriented through self exploration? What is your thought for how these systems integrate with the existing world?\nWhy does the OP's suggested mode of integration make you think of those older systems?\nThat is, other than me using them to bounce ideas off of and create small snippets of code.\nI'd love to hear about this. I applied to YC WC 25 with research/insight/an initial researchy prototype built on top of GPT4+finetuning about something along this idea. Less powerful than you describe, but it also works without the human moderated KG.\nI know we absolutely have not, but I think we have reached the limit in terms of the Chatbot experience that ChatGPT is. For some reason the industry keeps trying to force the chatbot interface to do literally everything to the point that we now have inflated roles like \"Prompt Engineers\". This is to say that people suck at knowing what they want off the rip, and LLMs can't help with that if they're not integrated in technology in such a way where a solid foundation is built to allow the models to generate good output.\nLLMs and other big data models have incredible potential for things like security, medicine, and the power industry to name a few fields. I mean I was recently talking with a professor about his research in applying deep learning to address growing security concerns in cars on the road.\nThe application is far from reaching the ceiling.\nBut understanding how likely it is that we will (or will not) see a new models quickly and dramatically improve on what we have \"because scaling\" seems valuable context for everyone in ecosystem to make decisions.\nCould you define \"code changes\" because I feel that is a very vague accomplishment.\nThat's where I'd focus.\nI wasn’t able to get it do it with Anthropic or OpenAI chat completion APIs. Can someone explain why? I don’t think the 200K token window actually works, is it looking sequentially or is it really looking at the whole thing at once or something?\nImagine that our current capabilities are like the Model-T. There remains many improvements to be made upon this passenger transportation product, with RAG being a great common theme among them. People will use chatbots with much more permissive interfaces instead of clicking through menus.\nBut all of that’s just the start, the short term, the maturation of this consumer product; the really scary/exciting part comes when the technology reaches saturation, and opens up new possibilities for itself. In the Model-T metaphor, this is analogous to how highways have (arguably) transformed America beyond anyone’s wildest dreams, changing the course of various historical events (eg WWII industrialization, 60s & 70s white flight, early 2000s housing crisis) so much it’s hard to imagine what the country would look like without them. Now, automobiles are not simply passenger transportation, but the bedrock of our commerce, our military, and probably more — through ubiquity alone they unlocked new forms of themselves.\nFor those doubting my utopian/apocalyptic rhetoric, I implore you to ask yourself one simple question: why are so many experts so worried about AGI? They’ve been leaving in droves from OpenAI, and that’s ultimately what the governance kerfluffle there was. Hinton, a Turing award winner, gave up $$$ to doom-say full time. Why?\nMy hint is that if your answer involves less then a 1000 specialized LLMs per unified system, then you’re not thinking big enough.\nThis is a hint of something but a weak argument. Smart people are wrong all the time.\nFYI, I find this line of reasoning to be unconvincing both logically and by counter-example (\"why are so many experts so worried about the Y2K bug?\")\nPersonally, I don't find AI foom or AI doom predictions to be probable but I do think there are more convincing arguments for your position than you're making here.\nFor us optimistic doomers, the AI conversation seems similar to the (early-2000s) climate change debate; we see a wave of dire warnings coming from scientific experts that are all-to-often dismissed, either out of hand due to their scale, or on the word of an expert in an adjacent-ish field. Of course, there’s more dissent among AI researchers than there was among climate scientists, but I hope you see where I’m coming from nonetheless — it’s a dynamic that makes it hard to see things from the other side, so-to-speak.\nAt this point I’ve pretty much given up convincing people on HackerNews, it’s just cathartic to give my piece and let people take it or leave it. If anyone wants to bring the convo down from industry trends into technical details, I’d love to engage tho :)\nhttps://chrisfrewin.medium.com/why-llms-will-never-be-agi-70...\nStill have like 2-3 big posts to publish.\nLong story short its easy to get enamored with an agent spitting out tokens out but reality and engineering are far far more complex than that (orders of magnitude)\nName your platform. Linux. C++. The Internet. The x86 processor architecture. We haven't exhausted the options for delivering value on top of those, but that doesn't mean the developers and sellers of those platforms don't try to improve them anyway and might struggle to extract value from application developers who use them.\nThe best engineering minds have been focused on scaling transformer pre and post training for the last three years because they had good reason to believe it would work, and it has up until now.\nProgress has been measured against benchmarks which are / were largely solvable with scale.\nThere is another emerging paradigm which is still small(er) scale but showing remarkable results. That's full multi-modal training with embodied agents (aka robots). 1x, Figure, Physical Intelligence, Tesla are all making rapid progress on functionality which is definitely beyond frontier LLMs because it is distinctly different.\nOpenAI/Google/Anthropic are not ignorant of this trend and are also reviving or investing in robots or robot-like research.\nSo while Orion and Claude 3.5 opus may not be another shocking giant leap forward, that does not mean that there arn't giant shocking leaps forward coming from slightly different directions.\nSure, that's tautologically true but that doesn't imply that beyondness will lead to significant leaps that offer notable utility like LLMs. Deep Learning overall has been a way around the problem that intelligent behavior is very hard to code and no wants to hire many, many coders needed to do this (and no one actually how to get a mass of programmers to actually be useful beyond a certain of project complexity, to boot). People take the \"bitter lesson\" to mean data can do anything but I'd say a second bitter lesson is that data-things are the low hanging fruit.\nMoreover, robot behavior is especially to fake. Impressive robot demos have been happening for decades without said robots getting the ability to act effectively in the complex, ad-hoc environment that human live in, IE, work with people or even cheaply emulate human behavior (but they can do choreographed/puppeteered kung fu on stage).\nEven assuming the recent robot demo was entirely AI, the only single thing they demonstrated that would have been noteworthy was isolating one voice in a noisy crowd well enough to respond; everything else I saw Optimus do, has already been demonstrated by others.\nWhat makes the uncertainty extra sad, is that a remote controllable humanoid robot is already directly useful for work in hazardous environments, and we know they've got at least that… but Musk would rather it be about the AI.\nCool, but we already have robots doing this in 2d space (aka self driving cars) that struggle not to kill people. How is adding a third dimension going to help? People are just refusing to accept the fact that machine learning is not intelligence.\nIf we have robots that operate in 3D, they'll be able to kill you not only from behind or from the side, but also from above. So that's progress!\nHowever interpolation isn't reasoning. If we want to understand the motion of planets, we would start with a dataset of (x, y, z, t) coordinates and try to derive the law of motion. Imagine if someone simply interpolated the dataset and presented the law of gravity as an array of million coefficients (aka weights)? Our minds have to work with a very small operating memory that can hardly fit 10 coefficients. This constraint forces us to develop intelligence that compacts the entire dataset into one small differential equation. Btw, English grammar is the differential equation of English in a lot of ways: it tells what the local rules are of valid trajectories of words that we call sentences.\nTesla is selling this view for almost a decade now in self-driving - how their car fleet feeding training data is going to make them leaders in the area. I don't find it convincing anymore\nAt CoRL last week, the progress has noticeably plateaued. Roboticists notably were pessimistic that scaling laws will apply to robotics because of the embodiment issues.\nNor does it mean that there are! We've gotten into this habit of assuming that we're owed giant shocking leaps forward every year or so, and this wave of AI startups raised money accordingly, but that's never how any innovation has worked. We've always followed the same pattern: there's a breakthrough which causes a major shift in what's possible, followed by a few years of rapid growth as engineers pick up where the scientists left off, followed by a plateau while we all get used to the new normal.\nWe ought to be expecting a plateau, but Sam Altman and company have done their work well and have convinced many of us that this time it's different. This time it's the singularity, and we're going to see exponential growth from here on out. People want to believe it, so they do, and Altman is milking that belief for all it's worth.\nBut make no mistake: Altman has been telegraphing that he's eyeing the exit, and you don't eye the exit when you own a company that's set to continue exponentially increasing in value.\nCan you think of any specific examples? Not trying to express disbelief, just curious given that this is obviously not what he's intending to communicate so it would be interesting to examine what seemed to communicate it.\nThe lack of progress with self driving seems to indicate that Tesla has a serious problem with scaling. The investment in enormous compute resources is another red flag (if you run out of ideas, just use brute force). This points to a fundamental flaw in model architecture.\nIt's almost like saying \"we've already visited every place on Earth, surely Mars is just around the corner now\"\nhttps://en.m.wikipedia.org/wiki/Five_Years_(David_Bowie_song...\nThe best minds don't follow the herd.\nOr because the people running companies who have fooled investors into believing it will work can afford to pay said engineers life-changing amounts of money.\nFor example recently I asked it to generate some phrases for a list of words, along with synonym and antonym lists.\nThe phrases were generally correct and appropriate (some mistakes but that’s fine). The synonyms/antonyms were misaligned to the list (so strictly speaking all wrong) and were often incorrect anyway. I imagine it would be the same if you asked for definitions of a list of words.\nIf you ask it to correct it just generates something else which is often also wrong. It’s certainly superficially convincing in many domains but once you try to get it to do real work it’s wrong in subtle ways.\nThis is supported by both general observations and recently this tweet from an OpenAI engineer that Sam responded to and engaged ->\n\"scaling has hit a wall and that wall is 100% eval saturation\"\nWhich I interpert to mean his view is that models are no longer yielding significant performance improvements because the models have maxed out existing evaluation metrics.\nAre those evaluations (or even LLMs) the RIGHT measures to achieve AGI? Probably not.\nBut have they been useful tools to demonstrate that the confluence of compute, engineering, and tactical models are leading towards signifigant breathroughts in artificial (computer) intelligence?\nI would say yes.\nWhich in turn are driving the funding, power innovation, public policy etc needed to take that next step?\nI hope so.\n(1) https://x.com/willdepue/status/1856766850027458648\nThey are driving the shoveling of VC money into a furnace to power their servers.\nShould that money run dry before they hit another breakthrough \"AI\" popularity is going to drop like a stone. I believe this to be far more likely an outcome than AGI or even the next big breakthrough.\nBut when I hear that models are failing to meet expectations, I imagine what they're saying is that the researchers had some sort of eval in mind with room to grow and a target, and that the model in question failed to hit the target they had in mind.\nHonestly, problem with sentiments like these is on Twitter is that you can't tell if they're being sincere or just making a snarky, useless remark. Probably a mix of both.\nMeanwhile, the existing tech is such a step change that industry is going to need time to figure out how to effectively use these models. In a lot of ways it feels like the \"digitization\" era all over again - workflows and organizations that were built around the idea humans handled all the cognitive load (basically all companies older than a year or two) will need time to adjust to a hybrid AI + human model.\nThis exactly. And as history shows, no matter how much effort the current big LLM companies do they won't be able to grasp the best uses for their tech. We will see small players developing it even further. I'm thankful for the legendary blindness of these anticompetitive behemoths. Less than 2 decades ago: IBM Watson.\nAnything that has more memory and adequate compute will win the coming AI wars.\nAt the rate at which power consumption is growing now that the shortage of current gen cards has started to work itself out people are realizing they need a fleet of nuclear reactors to keep the data centers running. This is not something that's getting fix with the coming generation, if anything it's worse.\nGiven that this is the case, why can't this be analogously true of “AI” as well? There's plenty of reason to believe that we're hitting a wall, such that, to progress further, said wall must be overcome by means of one or more breakthroughs.\nRight. If you generate some code with ChatGPT, and then try to find similar code on the web, you usually will. Search for unusual phrases in comments and for variable names. Often, something from Stack Overflow will match.\nLLMs do search and copy/paste with idiom translation and some transliteration. That's good enough for a lot of common problems. Especially in the HTML/Javascript space, where people solve the same problems over and over. Or problems covered in textbooks and classes.\nBut it does not look like artificial general intelligence emerges from LLMs alone.\nThere's also the elephant in the room - the hallucination/lack of confidence metric problem. The curse of LLMs is that they return answers which are confident but wrong. \"I don't know\" is rarely seen. Until that's fixed, you can't trust LLMs to actually do much on their own. LLMs with a confidence metric would be much more useful than what we have now.\nPeople who \"follow\" AI, as the latest fad they want to comment on and appear intelligent about, repeat things like this constantly, even though they're not actually true for anything but the most trivial hello-world types of problems.\nI write code all day every day. I use Copilot and the like all day every day (for me, in the medical imaging software field), and all day every day it is incredibly useful and writes nearly exactly the code I would have written, but faster. And none of it appears anywhere else; I've checked.\nWhat they fail at is code with high cyclomatic complexity. Back in the llama 2 finetune days I wrote a script that would break down what each node in the control flow graph into its own prompt using literate programming and the results were amazing for the time. Using the same prompts I'd get correct code in every language I tried.\nUm.\nAll the parent post said was:\n> then try to find similar code on the web, you usually will.\nNot identical code. Similar code.\nI think you're really stretching the domain of plausibility to suggest that any code you write is novel enough that you can't find 'similar' code on the internet.\nTo suggest that code generated from a corpus that is not going to be 'similar' to the code from the corpus is just factually and unambiguously false.\nOf course, it depends on what you interpret 'similar' to mean; but I think it's not unfair to say a lot of code is composed of smaller parts of code that is extremely similar to other examples of code on the internet.\nObviously you're not going to find an example similar to your entire code base; but if you're using, for example, copilot where you generate many small snippets of code... welll....\nBy that logic what you wrote was also composed that way. After all, you’ve used all words that have been used before! I bet even phrases like “that is extremely similar” and “generated from a corpus” and “unambiguously false”.\nAgain, I really find it hard to believe that anyone could make an argument like the one you’re making who has actually used these tools in their work for hundreds of hours, vs. for a couple minutes here or there with made up problems.\nWhat's true and what's not true is not related to what you personally believe.\nIt is factually and unambiguously false to state that generated code is, in general, not similar to other code from the corpus it is trained on.\n> And none of it appears anywhere else; I've checked.\n^ Even if this statement, is not false (I'm skeptical, but whatever), in general, it would be false for most users of copilot.\nNone of it appears anywhere else? None of it? Really?\nThat's not true of the no-AI code base I'm working on.\nThat's very difficult to believe it would be true on a code base heavily written by copilot and the like.\nIt's probably not true, in general, for AI generated code bases.\nWe can have a different conversation about verbatim copied code, where an AI model generates a large body of verbatim copy from a training source. That's very unusual.\n...but to say the generated code wouldn't even be similar? Come on.\nThat's literally what LLMs do.\nIn general, this is not a good description about what is happening inside an LLM. There is extensive literature on interpretability. It is complicated and still being worked out.\nThe commenter above might characterize the results they get in this way, but I would question the validity of that characterization, not to mention its generality.\nThere was another one that claimed to get rid of hallucinations. They also said it takes 50-100 epochs for regular architectures to actually memorize something. Their paper is below in case people qualified to review it want to.\nhttps://arxiv.org/abs/2406.17642\nLike the brain, I believe the problem will be solved by a mix of specialized components working together. One of those components will be a memory (or series of them) that the others reference to keep processing grounded in reality.\n[1] https://x.com/sama/status/1856941766915641580\nSo the models' accuracies won't grow exponentially, but can still grow linearly with the size of the training data.\nSounds like DataAnnotation will be sending out a lot more LinkedIn messages.\nEDIT: here's the paper https://arxiv.org/abs/2404.04125\nI probably disagree, but I don't want to criticize my interpretation of this sentence. Can you make your claim more precise?\nHere are some possible claims and refutations:\n- Claim: An LLM cannot output a true claim that it has not already seen. Refutation: LLMs have been shown to do logical reasoning.\n- Claim: An LLM cannot incorporate data that it hasn't been presented with. Refutation: This is an unfair standard. All forms of intelligence have to sense data from the world somehow.\n1. more data gets walled-off as owners realise value\n2. stackoverflow-type feedback loops cease to exist as few people ask a public question and get public answers ... they ask a model privately and get an answer based on last visible public solutions\n3. bad actors start deliberately trying to poison inputs (if sites served malicious responses to GPTBot/CCBot crawlers only, would we even know right now?)\n4. more and more content becomes synthetically generated to the point pre-2023 physical books become the last-known-good knowledge\n5. goverments and IP lawyers finally catch up\nWhat's amazing to me to is that no one is throwing accusations of plagiarism.\nI still think that if the \"wrong people\" had tried doing this they would have been obliterated by the courts.\nAnd our current AI is just pattern based intelligence based off of all human intelligence, some of that not being real intelligent data sources\nWhy do you think \"they\" have run out of data? First, to be clear, who do you mean by \"they\"? The world is filled with information sources (data aggregators for example), each available to some degree for some cost.\nDon't forget to include data that humans provide while interacting with chatbots.\nIn theory, yes you could generate an unlimited amount of data for the models, but how much of it is unique or valuable information? If you were to compress all this generated training data using a really good algorithm, how much actual information remains?\n... that being said I'm sure there is plenty of additional \"real data\" that hasn't been fed to these models yet. For one thing, I think ChatGPT sucks so bad at terraform because almost all the \"real code\" to train on is locked behind private repositories. There isn't much publicly available real-world terraform projects to train on. Same with a lot of other similar languages and tools -- a lot of that knowledge is locked away as trade secrets and hidden in private document stores.\n(that being said Sonnet 3.5 is much, much, much better at terraform than chatgpt. It's much better at coding in general but it's night and day for terraform)\nJust increase the temperature.\nOn the other hand, a lot of these frameworks and languages have relatively decent and detailed documentation.\nPerhaps this is a naive question, but why can't I as a user just purchase \"AI software\" that comes with a large pre-trained model to which I can say, on my own machine, \"go read this documentation and help me write this app in this next version of Leptos\", and it would augment its existing model with this new \"knowledge\".\n1. Find more data.\n2. Make the weights capture the data and reproduce.\nIn that sense we have reached a limit. So in my opinion we can do a couple of things.\n1. App developers can understand the limits and build within the limits.\n2. Researchers can take insights from these large models and build better AI systems with new architectures. It's ok to say transformers have reached a limit.\nI'm surprised that any of these companies consider what they are working on to be Artificial General Intelligences. I'm probably wrong, but my impression was AGI meant the AI is self aware like a human. An LLM hardly seems like something that will lead to self-awareness.\nFor example, in this article it says it can't do coding exercises outside the training set. That would definitely be on the \"AGI checklist\". Basically doing anything that is outside of the training set would be on that list.\nI will get excited for/scared of LLMs when they can tackle this kind of problem. But I don't believe they can because of the fundamental nature of their design, which is both backward looking (thus not better than the human state of the art) and lacks human intuition and self awareness. Or perhaps rather I believe that the prompt that would be required to get an LLM to produce such a program is a problem of at least equivalent complexity to implementing the program without an LLM.\nI've personally had some mild success getting these UTM variants to output their own children in a meta programming arrangement. The base program only has access to the valid instruction set of ~12 instructions per byte, while the task program has access to the full range of instructions and data per byte (256). By only training the base program, we reduce the search space by a very substantial factor. I think this would be similar to the idea of a self-hosted compiler, etc. I don't think there would be too much of a stretch to give it access to x86 instructions and a full VM once a certain amount of bootstrapping has been achieved.\n[0]: https://arxiv.org/abs/2406.19108\n[1]: https://github.com/kurtjd/brainfuck-evolved\n[2]: https://news.ycombinator.com/item?id=36120286\nThat’s possible for a highly intelligent, extensively trained, very small subset of humans.\nThat also ignores the fact that the small set of humans capable of building programming languages and compilers is a consequence of specialization and lack of interest. There are plenty of humans that are capable of learning how to do it. LLMs, on the other hand, are both specialized for the task and aren't lazy or uninterested.\nThings like drive a car, fold laundry, run an errand, do some basic math.\nYou'll notice that two of those require some form of robot or mobility. I think that is key -- you can't have AGI without the ability to interact with the world in a way similar to most humans.\nA crucial element of AGI would be the ability to self-train on self-generated data, online. So it's not really AGI if there is a hard distinction between training and inference (though it may still be very capable), and it's not really AGI if it can't work its way through novel problems on its own.\nThe ability to immediately solve a problem it's never seen before is too high a bar, I think.\nAnd yes, my definition still excludes a lot of humans in a lot of fields. That's a bullet I'm willing to bite.\nThat's not true. Humans can learn.\nAn LLM is just a tool. If it can't do what you want then too bad.\n(That’s not to say that humans don’t tend to lose some of their flexibility over their individual lifetimes as well.)\nThe lifetime is the context window, the model/training is the DNA. A human in the moment isn't general intelligent, but a human over his lifetime is, the first is so much easier to try to replicate though but that is a bad target since humans aren't born like that.\nhttps://plato.stanford.edu/entries/chinese-room/\nYet, one common assumption of many people running these companies or investing in them, or of some developers investing their time in these technologies, is precisely that some sort of explosion of superintelligence is likely, or even inevitable.\nIt surely is possible, but stretching that to likely seems a bit much if you really think how imperfectly we understand things like consciousness and the mind.\nOf course there are people who have essentially religious reactions to the notion that there may be limits to certain domains of knowledge. Nonetheless, I think that's the reality we're faced with here.\nI think Searle's view was that:\n- while it cannot be dis-_proven_, the Chinese Room argument was meant to provide reasons against believing it\n- the \"it can't be proven until it happens\" part is misunderstanding: you won't know if it happens because the objective, externally available attributes don't indicate whether self-awareness (or indeed awareness at all) is present\n> while it cannot be dis-_proven_, the Chinese Room argument was meant to provide reasons against believing it\nYes, like Russell's teapot. I also think that's what Searle means.\n> the \"it can't be proven until it happens\" part is misunderstanding: you won't know if it happens because the objective, externally available attributes don't indicate whether self-awareness (or indeed awareness at all) is present\nYes, agreed, I believe that's what Searle is saying too. I think I was maybe being ambiguous here - I wanted to say that even if you forgave the AI maximalists for ignoring all relevant philosophical work, the notion that \"appearing human-like\" inevitably tends to what would actually be \"consciousness\" or \"intelligence\" is more than a big claim.\nSearle goes further, and I'm not sure if I follow him all the way, personally, but it's a side point.\nDepends on how you define “self awareness” but knowing that it doesn't know something instead of hallucinating a plausible-but-wrong is already self awareness of some kind. And it's both highly valuable and beyond current tech's capability.\nhttps://openai.com/index/introducing-simpleqa/\nespecially this section Using SimpleQA to measure the calibration of large language models\nI'm wondering wether it would count, if one would extend it with an external program, that gives it feedback during inference (by another prompt) about the correctness of it's output.\nI guess it wouldn't, because these RAG tools kind of do that and i heard no one calling those self aware.\nThat is definitely an ability that current LLMs lack.\nIs that \"intelligent\" or \"understanding\"? It's probably close enough for pop science, and regardless, it looks good in headlines and sales pitches so why fight it?\n\"Artificial General Intelligence (AGI) refers to a theoretical form of artificial intelligence that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks at a level comparable to that of a human being.\"\nAltman says AGI could be here in 2025: https://youtu.be/xXCBz_8hM9w?si=F-vQXJgQvJKZH3fv\nBut he certainly means an LLM that can perform at/above human level in most tasks rather than a self aware entity.\n\"Most people\" naturally associate AGI with the sci-tropes of self-aware human-like agents.\nBut industries want something more concrete and prospectively-acheivable in their jargon, and so that's where AGI gets redefined as wide task suitability.\nAnd while that's not an unreasonable definition in the context of the industry, it's one that vanishingly few people are actually familiar with.\nAnd the commercial AI vendors benefit greatly from allowing those two usages to conflate in the minds of as many people as possible, as it lets them suggest grand claims while keeping a rhetorical \"we obviously never meant that!\" in their back pocket\nThe term itself (AGI) in the industry has always been about wide task suitability. People may have added their ifs and buts over the years but that aspect of it never got 'redefined'. The earliest uses of the term all talk about how well a machine would be able to perform some set number of tasks at some threshold.\nIt's no wonder why. Terms like \"consciousness\" and \"self-awareness\" are completely useless. It's not about difficulty. It's that you can't do anything at all with those terms except argue around in circles.\nBecause of that, the discussion of what AGI means in its broadest sense, will never end.\nSo in fact such AGI discussion will not make nobody wiser.\nI would argue that learning is The definition of AGI, since everything else comes naturally from that.\nThe current architectures can't learn without retraining, fine tuning is at the expense of general knowledge, and keeping things in context is detrimental to general performance. Once you have few shot learning, I think it's more of a \"give it agency so it can explore\" type problem.\nWhat does this mean? If I have a blind, deaf, paralyzed person, who could only communicate through text, what would the signs be that they were self aware?\nIs this more of a feedback loop problem? If I let the LLM run in a loop, and tell it it's talking to itself, would that be approaching \"self aware\"?\n(And by limitations I don’t mean “sorry, I’m not allowed to help you with this dangerous/contentious topic”.)\nI think this behavior is being somewhat demonstrated in newer models. I've seen GPT-3.5 175B correct itself mid response with, almost literally:\n> <answer with flaw here>\n> Wait, that's not right, that <reason for flaw>.\n> <correct answer here>.\nLater models seem to have much more awareness of, or \"weight\" towards, their own responses, while generating the response.\nYou won't get an LLM outputting \"wait, that's not right\" halfway through their original output (unless you prompted them in a way that would trigger such a speech pattern), because no re-evaluation is taking place without further input.\nThen there are those who are simply narcissistic, and cannot and will not admit fault regardless of the evidence presented them.\nNah, at best we found a way to make one part of a collection of systems that will, together, do something like thinking. Thinking isn’t part of what this current approach does.\nWhat’s most surprising about modern LLMs is that it turns out there is so much information statistically encoded in the structure of our writing that we can use only that structural information to build a fancy Plinko machine and not only will the output mimic recognizable grammar rules, but it will also sometimes seem to make actual sense, too—and the system doesn’t need to think or actually “understand” anything for us to, basically, usefully query that information that was always there in our corpus of literature, not in the plain meaning of the words, but in the structure of the writing.\nThis seems like the most viable path to me as well (educational background in neuroscience but don't work in the field). The brain is composed of many specialised regions which are tuned for very specific tasks.\nLLMs are amazing and they go some way towards mimicking the functionality provided by Broca's and Wernicke's areas, and parts of the cerebrum, in our wetware, however a full brain they do not make.\nThe work on robots mentioned elsewhere in the thread is a good way to develop cerebellum like capabilities (movement/motor control), and computer vision can mimic the lateral geniculate nucleus and other parts of the visual cortex.\nIn nature it takes all these parts working together to create a cohesive mind, and it's likely that an artificial brain would also need to be composed of multiple agents, instead of just trying to scale LLMs indefinitely.\nIt doesn't matter if that's happening or not. That's the whole point of the Chinese room - if it can look like it's understanding, it's indistinguishable from actually understanding. This applies to humans too. I'd say most of our regular social communication is done in a habitual intuitive way without understanding what or why we're communicating. Especially the subtle information conveyed in body language, tone of voice, etc. That stuff's pretty automatic to the point that people have trouble controlling it if they try. People get into conflicts where neither person understands where they disagree but they have emotions telling them \"other person is being bad\". Maybe we have a second consciousness we can't experience and which truly understands what it's doing while our conscious mind just uses the results from that, but maybe we don't and it still works anyway.\nEducators have figured this out. They don't test students' understanding of concepts, but rather their ability to apply or communicate them. You see this in school curricula with wording like \"use concept X\" rather than \"understand concept X\".\nI agree that a hypothetical perfectly-functioning Chinese room is, tautologically, impossible to distinguish from a real person who speaks Chinese, but that’s a thought experiment, not something that can actually exist. There’ll remain places where the “behavior” breaks down in ways that would be surprising from a human who’s actually paying as much attention as they’d need to be to have been interacting the way they had been until things went wrong.\nThat, in fact, is exactly where the difference lies: the LLM is basically always not actually “paying attention” or “thinking” (those aren’t things it does) but giving automatic responses, so you see failures of a sort that a human might also exhibit when following a social script (yes, we do that, you’re right), but not in the same kind of apparently-highly-engaged context unless the person just had a stroke mid-conversation or something—because the LLM isn’t engaged, because being-engaged isn’t a thing it does. When it’s getting things right and seeming to be paying a lot of attention to the conversation, it’s not for the same reason people give that impression, and the mimicking of present-ness works until the rule book goes haywire and the ever-gibbering player-piano behind it is exposed.\nThat's an interesting angle. Though of course we're not surprised by human behavior because that's where our expectations of understanding come from. If we were used to dealing with perfectly-correctly-understanding super-intelligences, then normal humans would look like we don't understand much and our deliberate thinking might be no more accurate than the super-intelligence's absent-minded automatic responses. Thus we would conclude that humans are never really thinking or understanding anything.\nI agree that default LLM output makes them look like they're thinking like a human more than they really are. I think mistakes are shocking more because our expectation of someone who talks confidently is that they're not constantly revealing themselves to be an obvious liar. But if you take away the social cues and just look at the factual claims they provide, they're not obviously not-understanding vs humans are-understanding.\nBut even more, maybe consciousness is an invention of our 'explaining self', maybe everything is automatic. I'm convinced this discussion is and will stay philosophical and will never get any conclusion.\nA human doesn’t just confidently spew paragraphs legit-looking but entirely wrong crap, unless they’re trying to deceive or be funny—an LLM isn’t trying to do anything, though, there’s no motivation, it doesn’t like you (it doesn’t like—it doesn’t it, one might even say), sometimes it definitely will just give you a beautiful and elaborate lie simply because its rulebook told it to, in a context and in a way that would be extremely weird if a person did it.\nWhen I read stuff like this it makes me wonder if people are actually using any of the LLMs...\nThe Emperor has no clothes.\nWhat do you mean by novel? Almost all sentences it is prompted on are brand new and it mostly responds sensibly. Surely there's some generalization going on.\nThe sort of generalization these things can do seems to mostly be the trivial sort: substitution.\nYes, LLMs aren't very good at reasoning and have weird failure modes. But why is this evidence that its on the wrong path, and not that it just needs more development that builds on prior successes?\nSo if you present a novel problem it would need to be extremely simple, not something that you couldn't solve when drunk and half awake. Completely novel, but extremely simple. I think that's testable.\nAnyway the novel problems I’m talking about are extremely simple. Basically they’re variations on the “farmer, 3 animals, and a rowboat” problem. People keep finding trivial modifications to the problem that fool the LLMs but wouldn’t fool a child. Then the vendors come along and patch the model to deal with them. This is what I mean by whack-a-mole.\nSearle’s Chinese Room thought experiment tells us that enough games of whack-a-mole could eventually get us to a pretty good facsimile of reasoning without ever achieving the genuine article.\nLike, every time an LLM gets something right we assume they've seen it somewhere in the training data, and every time they fail we presume they haven't. But that may not always be the case, it's just extremely hard to prove it one way or the other unless you search the entire dataset. Ironically the larger the dataset, the more likely the model is generalizing while also making it harder to prove if it's really so.\nTo give a human example, in a school setting you have teachers tasked with figuring out that exact thing for students. Sometimes people will read the question wrong with full understanding and fail, while other times they won't know anything and make it through with a lucky guess. If LLMs (and their vendors) have learned anything it's that confidently bullshitting gets you very far which makes it even harder to tell in cases where they aren't. Somehow it's also become ubiquitous to tune models to never even say \"I don't know\" because it boosts benchmark scores slightly.\nBecause I had no idea how these were built until I read the paper, so couldn’t really tell what sort of tree they’re barking up. The failure-modes of LLMs and ways prompts affect output made a ton more sense after I updated my mental model with that information.\nThen on learning how it works, you might realize flapping just isn’t something they’re built to do, and it wouldn’t make much sense if they did flap their wings, given how they work instead.\nAnd yet—damn, they fly fast! That’s impressive, and without a single flap! Amazing. Useful!\nAt no point did their behavior change, but your ability to understand how and why they do what they do, and why they fail the ways they fail instead of the ways birds fail, got better. No more surprises from expecting them to be more bird-like than they are supposed to, or able to be!\nAnd now you can better handle that guy over there talking about how powerful and scary these “metal eagles” (his words) are, how he’s working so hard to make sure they don’t eat us with their beaks (… beaks? Where?), they’re so powerful, imagine these huge metal raptors ruling the sky, roaming and eating people as they please, while also… trying to sell you airplanes? Actively seeking further investment in making them more capable? Huh. One begins to suspect the framing of these things as scary birds that (spooky voice) EVEN THEIR CREATORS FEAR FOR THEIR BIRD-LIKE QUALITIES (/spooky voice) was part of a marketing gimmick.\nBut how do you know a magician that knows how to do card tricks isn't going to arrive at real magic? Shakes head.\nA future of people interacting with humanoid robots seems like cheesy sci-fi dream, same as a future of people flitting about in flying cars. However, if we really did want to create robots like this that took care not to damage themselves, and could empathize with human emotions, then we'd need to build a lot of this in, the same way that it's built into ourselves.\nLLMs already outperform humans in a huge variety of tasks. ML in general outperform humans in a large variety of tasks. Are all of them AGI? Doubtful.\nIf it acts like one, whether you call a machine conscious or not is pure semantics. Not like potential consequences are any less real.\n>LLMs already outperform humans in a huge variety of tasks.\nYes, LLMs are General Intelligences and if that is your only requirement for AGI, they certainly already are[0]. But the definition above hinges on long-horizon planning and competence levels that todays models have generally not yet reached.\n>ML in general outperform humans in a large variety of tasks.\nThis is what the G in AGI is for. Alphafold doesn't do anything but predict proteins. Stockfish doesn't do anything but play chess.\n>Are all of them AGI? Doubtful.\nWell no, because they're missing the G.\n[0] https://www.noemamag.com/artificial-general-intelligence-is-...\nThe \"hard problem\", to which you may be alluding, may never matter. It's already feasible for an 'AI/AGI with LLM component' to be \"self-aware\".\nWe use the term self-awareness as an all encompassing reference of our cognizant nature. It's much more than just having an internal model of self.\nInteresting essay enumerating reasons you may be correct: https://medium.com/@francois.chollet/the-impossibility-of-in...\nOr did you mean consciousness? How would one demonstrate that an AGI is conscious? Why would we even want to build one?\nMy understanding is an AGI is at least as smart as a typical human in every category. That is what would be useful in any case.\nhttps://en.m.wikipedia.org/wiki/Conway%27s_Game_of_Life\nIf it doesn’t lead to AGI, as an employee it’s not your problem.\nWhere do these large \"AI\" companies think the mass amounts of data used to train these models come from? People! The most powerful and compact complex systems in existence, IMO.\nIf it were one of many, I think you would name something better.\n[1] https://en.wikipedia.org/wiki/Coherence_theory_of_truth\nAlphaGo - self-play\nAlphaFold - PDB, the protein database\nChatGPT - human knowledge encoded as text\nThese models are all machines for clever interpolation in gigantic training datasets.\nThey appear to be intelligent, because the training data they've seen is so vastly larger than what we've seen individually, and we have poor intuition for this.\nI'm not throwing shade, I'm a daily user of ChatGPT and find tremendous and diverse value in it.\nI'm just saying, this particular path in AI is going to make step-wise improvements whenever new large sources of training data become available.\nI suspect the path to general intelligence is not that, but we'll see.\nI think there's three things that a 'true' general intelligence has which is missing from basic-type-LLMs as we have now.\n1. knowing what you know. <basic-LLMs are here>\n2. knowing what you don't know but can figure out via tools/exploration. <this is tool use/function calling>\n3. knowing what can't be known. <this is knowing that halting problem exists and being able to recognize it in novel situations>\n(1) From an LLM's perspective, once trained on corpus of text, it knows 'everything'. It knows about the concept of not knowing something (from having see text about it), (in so far as an LLM knows anything), but it doesn't actually have a growable map of knowledge that it knows has uncharted edges.\nThis is where (2) comes in, and this is what tool use/function calling tries to solve atm, but the way function calling works atm, doesn't give the LLM knowledge the right way. I know that I don't know what 3,943,034 / 234,893 is. But I know I have a 'function call' of knowing the algorithm for doing long divison on paper. And I think there's another subtle point here: my knowledge in (1) includes the training data generated from running the intermediate steps of the long-division algorithm. This is the knowledge that later generalizes to being able to use a calculator (and this is also why we don't just give kids calculators in elementary school). But this is also why a kid that knows how to do long division on paper, doesn't seperately need to learn when/how to use a calculator, besides the very basics. Using a calculator to do that math feels like 1 step, but actually it does still have all of initial mechanical steps of setting up the problem on paper. You have to type in each digit individually, etc.\n(3) I'm less sure of this point now that I've written out point (1) and (2), but that's kinda exactly the thing I'm trying to get at. Its being able to recognize when you need more practice of (1) or more 'energy/capital' for doing (2).\nConsider a burger resturant. If you properly populated the context of a ChatGPT-scale model the data for a burger resturant from 1950, and gave it the kinda 'function calling' we're plugging into LLMs now, it could manage it. It could keep track of inventory, it could keep tabs on the employee-subprocesses, knowing when to hire, fire, get new suppliers, all via function calling. But it would never try to become McDonalds, because it would have no model of the the internals of those function-calls, and it would have no ability to investigate or modify the behaviour of those function calls.\nLearning from data is not enough; there is a need for the kind of system-two thinking we humans develop as we grow. It is difficult to see how deep learning and backpropagation alone will help us model that. For tasks where providing enough data is sufficient to cover 95% of cases, deep learning will continue to be useful in the form of 'data-driven knowledge automation.' For other cases, the road will be much more challenging. https://www.lycee.ai/blog/why-sam-altman-is-wrong\nI wonder what this would mean for companies raising today on the premise of building on top of these platforms. Maybe the best ones get their ideas copied, reimplemented, and sold for cheaper?\nWe already kind of see this today with OpenAI's canvas and Claude artifacts. Perhaps they'll even start moving into Palantir's space and start having direct customer implementation teams.\nIt is becoming increasing obvious that LLM's are quickly becoming commoditized. Everyone is starting to approach the same limits in intelligence, and are finding it hard to carve out margin from competitors.\nMost recently exhibited by the backlash at claude raising prices because their product is better. In any normal market, this would be totally expected, but people seemed shocked that anyone would charge more than the raw cost it would take to run the LLM itself.\nhttps://x.com/ArtificialAnlys/status/1853598554570555614\nAmazon and Google didn't mess with their core business by competing with the players using it until they REALLY ran out of ways to make money.\nThis smells like it’s mostly based on OAI having a bit of bad luck with next model rather than a fundamental slowdown / barrier.\nThey literally just made a decent sized leap with o1\nThe Information reporting was a bit more clear on this. Orion is better than GPT-4, it's just that they were expecting a leap in capabilities comparable to what we saw going from GPT-3 to GPT-4. In other words, they were expecting essentially a GPT-5, and Orion wasn't that good.\nSo the problem is more in the algorithm.\nDefinitely cheaper.\nYes, because we understand the rough biological processes that cause this, and they are not remotely similar to this technology. We can also observe it. There is no evidence that current approaches can make LLM's achieve AGI, nor do we even know what processes would cause that.\nWe don't have a rough understanding of the biological processes that cause this, unless you literally mean just the biological process and not how it actual impacts learning/intelligence.\nThere's no evidence that we (brains) have achieved AGI, unless you tautologically define AGI as our brains.\nYes we do. We know how neurons communicate, we know how they are formed, we have great evidence and clues as to how this evolved and how our various neurological symptoms are able to interact with the world. Is it a fully solved problem? no.\n> unless you literally mean just the biological process and not how it actual impacts learning/intelligence.\nOf course we have some understanding of this as well. There's tremendous bodies of study around this. We know which regions of the brain correlate to reasoning, fear, planning, etc. We know when these regions are damaged or removed what happens, enough to point to a region of the brain and say \"HERE.\" That's far, far beyond what we know about the innards of LLM's.\n> here's no evidence that we (brains) have achieved AGI, unless you tautologically define AGI as our brains.\nThis is extremely circular because the current definition(s) of AGI always define it in terms of human intelligence. Unless you're saying that intelligence comes from somewhere other than our brains.\nAnyway, the brain is not like a LLM, in function or form, so this debate is extremely silly to me.\nIt's not even close to fully solved. We're still figuring out basic things like the purpose of dreams. We don't understand how memories are encoded or even things like how we process basic emotions like happiness. We're way closer to understanding LLMs than we are the brain, and we don't understand LLMs all that well still either. For example, look at the Golden Gate Bridge work for LLMs -- we have no equivalent for brains today. We've done much more advanced introspection work on LLMs in this short amount of time than we've done on the human brain.\nIs this certain? Are Agents the right direction to AGI?\nIt’s not true that any element, when duplicated and linked together will exhibit anything emergent. Neural networks (in a certain sense, though not their usual implementation) are already built out of individual units linked together, so simply having more of these groups of units might not add anything important.\n> research is already showing promising results of the performance of agent systems.\n…in which case, please show us! I’d be interested.\nIsn't that literally the cause of the success of deep learning? It's not quite \"free\", but as I understand it, the big breakthrough of AlexNet (and much of what came after) was that running a larger CNN on a larger dataset allowed the model to be so much more effective without any big changes in architecture.\nGoodbye, Mr. Anderson...\nCertain OpenAI insiders must have known this for a while, hence Ilya Sutskever's new company in Israel\nI was really looking forward to using \"synthetic data\" euphemistically during debates.\nAnd there's a number of reasons why, mostly likely being that they've found other ways to get improvements out of AI models, so diminishing returns on training aren't that much of a problem. Or, maybe the leakers are lying, but I highly doubt that considering the past record of news outlets reporting on accurate leaked information.\nStill though, it's interesting how basically ever frontier lab created a model that didn't live up to expectations, and every employee at these labs on Twitter has continued to vague-post and hype as if nothing ever happened.\nIt's honestly hard to tell whether or not they really know something we don't, or if they have an irrational exuberance for AGI bordering on cult-like, and they will never be able to mentally process, let alone admit, that something might be wrong.\nUp to a certain point, a conditional fluency stores knowledge, in the sense that semantically correct sentences are more likely to be fluent… but we may have tapped out in that regard. LLMs have solved language very well, but to get beyond that has seemed, thus far, to require RLHF, with all the attendant negatives.\nAnd I think the latter is good enough for us to do exciting things.\nThis might be acceptable for amusing us with fiction and art, and for filling the internet with even more spam and propaganda, but would you trust them to write reliable code, drive your car or control any critical machinery?\nThe truly exciting things are still out of reach, yet we just might be at the Peak of Inflated Expectations to see it now.\n[1]: https://openai.com/index/introducing-simpleqa/\nA lot hangs on what you mean by \"significant\". Can you define what you mean? And/or give an example of an improvement that you don't think is significant.\nAlso, on what basis can you say \"no significant improvements\" have been made? Many major players have published some of their improvements openly. They also have more private, unpublished improvements.\nIf your claim boils down to \"what people mean by a Generative Pre-trained Transformer\" still has a clear meaning, ok, fine, but that isn't the meat of the issue. There is so much more to a chat system than just the starting point of a vanilla GPT.\nIt is wiser to look at the whole end-to-end system, starting at data acquisition, including pre-training and fine-tuning, deployment, all the way to UX.\nP.S. I don't have a vested interest in promoting or disparaging AI. I don't work for a big AI lab. I'm just trying to call it like I see it, as rationally as I can.\nGoing from 10% to 50% (500% more) complete coverage of common sense knowledge and reasoning is going to feel like a significant advance. Going from 90% to 95% (5% more) coverage is not going to feel the same.\nRegardless of what Altman says, its been two years since OpenAI released GPT-4, and still no GPT-5 in sight, and they are now touting Q-star/strawberry/GPT-o1 as the next big thing instead. Sutskever, who saw what they're cooking before leaving, says that traditional scaling has plateaeud.\nIt's been 20 months since 4 was released. 3 was released 32 months after 2. The lack of a release by now in itself does not mean much of anything.\nSutskever, recently ex. OpenAI, one of the first to believe in scaling, now says it is plateauing. Do OpenAI have something secret he was unaware of? I doubt it.\nFWIW, GPT-2 and GPT-3 were about a year apart (2019 \"Language models are Unsupervised Multitask Learners\" to 2020 \"Language Models are Few-Shot Learners\").\nDario Amodei recently said that with current gen models pre-training itself only takes a few months (then followed by post-training, etc). These are not year+ training runs.\nBlind scaling sure (for whatever reason)* but this is the same Sutskever who believes in ASI within a decade off the back of what we have today.\n* Not like anyone is telling us any details. After all, Open AI and Microsoft are still trying to create a 100B data center.\nIn my opinion, there's a difference between scaling not working and scaling becoming increasingly infeasible. GPT-4 is something like x100 the compute of 3 (Same with 2>3).\nAll the drips we've had of 5 point to ~x10 of 4. Not small but very modest in comparison.\n>FWIW, GPT-2 and GPT-3 were about a year apart (2019 \"Language models are Unsupervised Multitask Learners\" to 2020 \"Language Models are Few-Shot Learners\").\nAh sorry I meant 3 and 4.\n>Dario Amodei recently said that with current gen models pre-training itself only takes a few months (then followed by post-training, etc). These are not year+ training runs.\nYou don't have to be training models the entire time. GPT-4 was done training in August 2022 according to Open AI and wouldn't be released for another 8 months. Why? Who knows.\nYes - it'll be interesting to see if there are any signs of these plans being adjusted. Apparently Microsoft's first step is to build optical links between existing data centers to create a larger distributed cluster, which must be less of a financial commitment.\nMeta seem to have an advantage here in that they have massive inference needs to run their own business, so they are perhaps making less of a bet by building out data centers.\nAt the very early phase of the boom I was among a very few who knew and predicted this (usually most free and deep thinking/knowledgeable). Then my prediction got reinforced by the results. One of the best examples was with one of my experiments that all today's AI's failed to solve tree serialization and de-serialization in each of the DFS(pre-order/in-order/post-order) or BFS(level-order) which is 8 algorithms (2x4) and the result was only 3 correct! Reason is \"limited training inputs\" since internet and open source does not have other solutions :-) .\nSo, I spent \"some\" time and implemented all 8, which took me few days. By the way this proves/demonstrates that ~15-30min pointless leetcode-like interviews are requiring to regurgitate/memorize/not-think. So, as a logical hard consequence there will.has-to be a \"crash/cleanup\" in the area of leetcode-like interviews as they will just be suddenly proclaimed as \"pointless/stupid\"). However, I decided not to publish the rest of the 5 solutions :-)\nThis (and other experiments) confirms hard limits of the LLM approach (even when used with chain-of-thought). Increasing the compute on the problem will produce increasingly smaller and smaller results (inverse exponential/logarithmic/diminishing-returns) = new AGI approach/design is needed and to my knowledge majority of the inve$tment (~99%) is in LLM, so \"buckle up\" at-some-point/soon?\nImpacts and realities; LLM shall \"run it's course\" (produce some products/results/$$$, get reviewed/$corrected) and whoever survives after that pruning shall earn money on those products while investing in the new research to find new AGI design/approach (which could take quite a long time,... or not). NVDA is at the center of thi$ and time-wise this peak/turn/crash/correction is hard to predict (although I see it on the horizon and min/max time can be estimated). Be aware and alert. I'll stop here and hold my other number of thoughts/opinions/ideas for much deeper discussion. (BTW I am still \"full in on NVDA\" until,....)\nKant describes two human “senses”: the intensive sense of time, and the extensive sense of space. In this paradigm, spatial experience would be inextricably tied to all forms of logic, because it helps train the cognitive faculties that are intrinsically tied to all complex (discriminative?) thought.\nIs it just me or does $100 million sound like it's on the very, very low end of how much training a new model costs? Maybe you can arrive within $200 million of that mark with amortization of hardware? It just doesn't make sense to me that a new model would \"only\" be $100 million when AmaGooBookSoft are spending tens of billions on hardware and the AI startups are raising billions every year or two.\nAGI=lim(x->0)AIHype(x)\nwhere x=length of winter\nhttps://gwern.net/gpt-3#bpes\nhttps://paperswithcode.com/paper/most-language-models-can-be...\nThe appearance of improvements in that capability are due to the vocabulary of modern LLMs increasing. Still only putting lipstick on a pig.\nAnd if your \"lipstick on a pig\" argument is that even when they generate haikus, they aren't really writing haikus, then I'll link to this other gwern post, about how they'll never really be able to solve the rubik's cube - https://gwern.net/rubiks-cube\nWith my user hat on, I'm quite pleased with the current state of LLMs. Initially, I approached them skeptically, using a hackish mindset and posing all kinds of Turing test-like questions. Over time, though, I shifted my focus to how they can enhance my team's productivity and support my own tasks in meaningful ways.\nFinally, I see LLMs as a valuable way to explore parts of the world, accommodating the reality that we simply don’t have enough time to read every book or delve into every topic that interests us.\nWatch this be a power move to break from Microsofts investment when ready rather than true agi. Sam is laying the foundations here.\nhttp://www.incompleteideas.net/IncIdeas/BitterLesson.html\nSo it's interesting that when AI came along, we threw caution to the wind and started treating it like a silver bullet... Without asking the question of whether it was applicable to this goal or that goal...\nI don't think anyone could have anticipated that we could have an AI which could produce perfect sentences, faster than a human, better than a human but which could not reason. It appears to reason very well, better than most people, yet it doesn't actually reason. You only notice this once you ask it to accomplish a task. After a while, you can feel how it lacks willpower. It puts into perspective the importance of willpower when it comes to getting things done.\nIn any case, LLMs bring us closer to understanding some big philosophical questions surrounding intelligence and consciousness.\nThe irony here is astounding.\nAI will always have a specific narrow focus and will never ever be creative, the best AI proponents can hope for is that the hallucinations will drop to a more unnoticable level.\nSometimes other outlets do copycat reporting of theirs, and those submissions are ok, though they wouldn't be if the original source were accessible.\nTo be clear, I don't think a near-term bubble collapse is likely but I'm going from 3% to maybe ~10%. Also, this doesn't mean I doubt there's real long-term value to be delivered or money to be made in AI solutions. I'm thinking specifically about those who've been speculatively funding the massive build out of data centers, energy and GPU supply expecting near-term demand to continue scaling at the recent unprecedented rates. My understanding is much of this is being funded in advance of actual end-user demand at these elevated levels and it is being funded either by VC money or debt by parties who could struggle to come up with the cash to pay for what they've ordered if either user demand or their equity value doesn't continue scaling as expected.\nAdmittedly this scenario assumes that these investment commitments are sufficiently speculative and over-committed to create bubble dynamics and tipping points. The hypothesis goes like this: the money sources who've over-committed to lock up scarce future supply in the expectation it will earn outsize returns have already started seeing these warning signs of efficiency and/or progress rates slowing which are now hitting mainstream media. Thus it's possible there is already a quiet collapse beginning wherein the largest AI data center GPU purchasers might start trying to postpone future delivery schedules and may soon start trying to downsize or even cancel existing commitments or try to offload some of their future capacity via sub-leasing it out before it even arrives, etc. Being a dynamic market, this could trigger a rapidly snowballing avalanche of falling prices for next-year AI compute (which is already bought and sold as a commodity like pork belly futures).\nNotably, there are now rumors claiming some of the largest players don't currently have the cash to pay for what they've already committed to for future delivery. They were making calculated bets they'd be able to raise or borrow that capital before payments were due. Except if expectation begins to turn downward, fresh investors will be scarce and banks will reprice a GPU's value as loan collateral down to pennies on the dollar (shades of the 2009 financial crisis where the collateral value of residential real estate assets was marked down). As in most bubbles, cheap credit is the fuel driving growth and that credit can get more expensive very quickly - which can in turn trigger exponential contagion effects causing the bubble to pop. A very different kind of \"Foom\" than many AI financial speculators were betting on! :-)\nSo... in theory, under this scenario sometime next year NVidia/TSMC and other top-of-supply-chain companies could find themselves with excess inventories of advanced node wafers because a significant portion of their orders were from parties who no longer have access to the cheap capital to pay for them. And trying to sue so many customers for breach can take a long time and, in a large enough sector collapse, be only marginally successful in recouping much actual cash.\nI'd be interested in hearing counter-arguments (or support) for the impossibility (or likelihood) of such a scenario.\nOn the other hand, selling to customers who can't pay but who look solvent to public investors sounds like the kind of short-termism nobody should be too surprised to be reading a book about in a few years...\n[1] https://dl.acm.org/doi/10.1145/3442188.3445922\nI don't get it...\n> I suspect the path to general intelligence is not that, but we'll see.\nThat doesn't mean this article is irrelevant. It's good to know if LLM improvements are going to slow down a bit because the low hanging fruit has seemingly been picked.\nBut in terms of the overall effect of AI and questioning the validity of the technology as a whole, it's just your basic FUD article that you'd expect from mainstream news.\nAm I missing something? I thought general consensus was that Moore's Law in fact did die:\nhttps://cap.csail.mit.edu/death-moores-law-what-it-means-and...\nThe fact that we've still found ways to speed up computations doesn't obviate that.\nWe've mostly done that by parallelizing and applying different algorithms. IIUC that's precisely why graphics cards are so good for LLM training - they have highly-parallel architectures well-suited to the problem space.\nAll that seems to me like an argument that LLMs will hit a point of diminishing returns, and maybe the article gives some evidence we're starting to get there.\nThe article you pointed out says the end came in 2016: Eight years ago.\nMy point is those types of articles have been popping up every few years since the 1990s. Sure, at some point these sort of predictions will be proven correct about LLMs as well. Probably in a few decades.\nxd\nIMO this will require not just much more expansive multi-modal training, but also novel architecture, specifically, recurrent approaches; plus a well-known set of capabilities most systems don't currently have, e.g. the integration of short-term memory (context window if you like) into long-term \"memory\", either episodic or otherwise.\nBut these are as we say me",
    "highlightText": "",
    "highlightTitle": "",
    "highlightThreadTitle": "",
    "language": "english",
    "sentiment": "negative",
    "categories": [
        "Science and Technology"
    ],
    "ai_allow": true,
    "canonical": false,
    "webz_reporter": false,
    "external_links": [
        "https://en.m.wikipedia.org/wiki/Conway%27s_Game_of_Life",
        "https://arxiv.org/abs/2404.04125",
        "https://www.lycee.ai/blog/why-sam-altman-is-wrong",
        "https://dl.acm.org/doi/10.1145/3442188.3445922",
        "https://plato.stanford.edu/entries/chinese-room/",
        "https://openai.com/index/introducing-simpleqa/",
        "https://en.wikipedia.org/wiki/Coherence_theory_of_truth",
        "https://x.com/ArtificialAnlys/status/1853598554570555614",
        "http://www.incompleteideas.net/IncIdeas/BitterLesson.html",
        "https://github.com/kurtjd/brainfuck-evolved",
        "https://lnkd.in/duDST65P)",
        "https://muldoon.cloud/2023/10/29/ai-commandments.html",
        "https://arxiv.org/abs/2406.19108",
        "https://www.biorxiv.org/content/10.1101/2024.07.01.600583v1",
        "https://x.com/sama/status/1856941766915641580",
        "https://manifold.markets/ai",
        "https://lnkd.in/ddvuzaYp",
        "https://youtu.be/xXCBz_8hM9w?si=F-vQXJgQvJKZH3fv",
        "https://x.com/willdepue/status/1856766850027458648",
        "https://github.com/oobabooga/text-generation-webui",
        "https://arxiv.org/abs/2406.17642",
        "https://gwern.net/gpt-3#bpes",
        "https://gwern.net/rubiks-cube",
        "https://www.arxiv.org/abs/2406.17642",
        "https://www.gwern.net/gpt-3#bpes",
        "https://www.arxiv.org/abs/2404.04125",
        "https://lycee.ai/blog/why-sam-altman-is-wrong",
        "https://www.gwern.net/rubiks-cube",
        "https://biorxiv.org/content/10.1101/2024.07.01.600583v1",
        "https://www.x.com/willdepue/status/1856766850027458648",
        "https://openai.com/index/introducing-simpleqa",
        "https://www.github.com/kurtjd/brainfuck-evolved",
        "https://www.dl.acm.org/doi/10.1145/3442188.3445922",
        "https://www.x.com/ArtificialAnlys/status/1853598554570555614",
        "https://www.arxiv.org/abs/2406.19108",
        "https://www.youtu.be/xXCBz_8hM9w?si=F-vQXJgQvJKZH3fv",
        "https://www.github.com/oobabooga/text-generation-webui",
        "https://www.x.com/sama/status/1856941766915641580",
        "https://www.lnkd.in/duDST65P)",
        "https://www.en.m.wikipedia.org/wiki/Conway%27s_Game_of_Life",
        "https://www.openai.com/index/introducing-simpleqa/",
        "http://incompleteideas.net/IncIdeas/BitterLesson.html",
        "https://www.lnkd.in/ddvuzaYp",
        "https://www.plato.stanford.edu/entries/chinese-room/",
        "https://www.muldoon.cloud/2023/10/29/ai-commandments.html",
        "https://www.en.wikipedia.org/wiki/Coherence_theory_of_truth",
        "https://plato.stanford.edu/entries/chinese-room",
        "https://youtu.be/xXCBz_8hM9w",
        "https://www.manifold.markets/ai"
    ],
    "external_images": [],
    "entities": {
        "persons": [],
        "organizations": [
            {
                "name": "Google",
                "sentiment": "negative"
            }
        ],
        "locations": []
    },
    "syndication": {
        "syndicated": false,
        "syndicate_id": null,
        "first_syndicated": false
    },
    "rating": null,
    "crawled": "2024-11-15T09:09:46.841+02:00",
    "updated": "2024-11-15T09:09:46.841+02:00"
}