{
    "thread": {
        "uuid": "fac5647a575d0f53e47b9d8a964d9d61dbaa0d72",
        "url": "https://www.eff.org/deeplinks/2024/08/why-weak-guardrails-police-face-recognition-use-may-actually-make-things-worse",
        "site_full": "www.eff.org",
        "site": "eff.org",
        "site_section": "http://www.eff.org/rss/updates.xml",
        "site_categories": [
            "tech"
        ],
        "section_title": "Deeplinks",
        "title": "Weak \"Guardrails\" on Police Face Recognition Use Make Things Worse",
        "title_full": "Weak \"Guardrails\" on Police Face Recognition Use Make Things Worse",
        "published": "2024-08-09T00:07:00.000+03:00",
        "replies_count": 0,
        "participants_count": 1,
        "site_type": "news",
        "country": "US",
        "main_image": "https://www.eff.org/files/banner_library/whyfacebanner.png",
        "performance_score": 0,
        "domain_rank": 400,
        "domain_rank_updated": "2024-08-06T13:07:25.000+03:00",
        "reach": null,
        "social": {
            "facebook": {
                "likes": 0,
                "comments": 0,
                "shares": 3
            },
            "gplus": {
                "shares": 0
            },
            "pinterest": {
                "shares": 0
            },
            "linkedin": {
                "shares": 0
            },
            "stumbledupon": {
                "shares": 0
            },
            "vk": {
                "shares": 0
            }
        }
    },
    "uuid": "fac5647a575d0f53e47b9d8a964d9d61dbaa0d72",
    "url": "https://www.eff.org/deeplinks/2024/08/why-weak-guardrails-police-face-recognition-use-may-actually-make-things-worse",
    "ord_in_thread": 0,
    "parent_url": null,
    "author": "Hayley Tsukayama",
    "published": "2024-08-09T00:07:00.000+03:00",
    "title": "Weak \"Guardrails\" on Police Face Recognition Use Make Things Worse",
    "text": "Police use of face recognition technology (FRT) poses a particularly massive risk to our civil liberties, [particularly for Black men and women](https://www.scientificamerican.com/article/police-facial-recognition-technology-cant-tell-black-people-apart/) and other marginalized communities. That's why EFF supports a ban on government FRT use. Half-measures aren't up to the task. However, even as half-measures go, California's legislature is currently considering a [particularly weak proposal](https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240AB1814) in the form of A.B. 1814, authored by Asm. Phil Ting (San Francisco). It would introduce paltry limits that will do nothing to address the many problems that police use of face recognition raises. In fact, the bill's language could make things worse in California. This something? It's worse than nothing—by a long shot. For example, major police departments in California [have pledged not to use](https://onezero.medium.com/lapd-drops-clearview-a-i-but-not-all-facial-recognition-a5e7931e40bc) Clearview AI, a company that's [been sued repeatedly](https://www.eff.org/deeplinks/2022/02/victory-another-lawsuit-proceeds-against-clearviews-face-surveillance) for building its database from scraped social media posts, in light of public pressure. But A.B. 1814 expressly gives police departments the right to access \"third-party databases,\" including Clearview AI. This could give law enforcement agencies cover to use databases that they have previously distanced themselves from and will erode progress civil liberties advocates have already made. The bill also states police have access to any state database, even if the images were not collected for law enforcement purposes. California should not give law enforcement the green light to mine databases, particularly those built for completely different reasons. This goes against what people are expecting when they give their information to one database, only to learn later that information has been informing police face surveillance. Finally, A.B. 1814 fails to even meet the bar of restrictions other police departments have agreed to adopt. As we have previously written, the [Detroit Police Department](https://www.eff.org/deeplinks/2024/07/detroit-takes-important-step-curbing-harms-face-recognition-technology) agreed to limits on its use of face recognition technology after it [falsely arrested Robert Williams](https://www.eff.org/deeplinks/2021/10/face-recognition-isnt-just-face-identification-and-verification) as a result of a incorrect face recognition \"match.\" As part of these limits, the Detroit police have agreed to \"bar arrests based solely on face recognition results, or the results of the ensuing photo lineup.\" Their agreement also affirms that prosecutors and defense attorneys will have access to information about any uses of FRT in cases where law enforcement files charges. The California bill does not even include these safeguards. It says that police can not use a database match as the sole basis for an arrest, but it would allow a photo lineup based on a match to be considered a second technique. This puts people who look like the suspect in front of witnesses who may be likely to pick that person—even if it is an entirely different person. That lets law enforcement agencies easily clear the low bar the bill sets. A.B. 1814 is sitting in the Senate Appropriations Committee. EFF has joined with dozens of civil liberties organizations to urge the committee not to advance the bill. If it does move forward, we'll be asking you to help us fight it on the Senate floor. Proponents of the bill have argued that, essentially, it is better to do something than have no guardrails in place. But this something? It's worse than nothing—by a long shot.",
    "highlightText": "",
    "highlightTitle": "",
    "highlightThreadTitle": "",
    "language": "english",
    "sentiment": "negative",
    "categories": [
        "Crime, Law and Justice",
        "Social Issue"
    ],
    "ai_allow": true,
    "webz_reporter": false,
    "external_links": [
        "https://www.scientificamerican.com/article/police-facial-recognition-technology-cant-tell-black-people-apart/",
        "https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240AB1814",
        "https://onezero.medium.com/lapd-drops-clearview-a-i-but-not-all-facial-recognition-a5e7931e40bc",
        "https://www.onezero.medium.com/lapd-drops-clearview-a-i-but-not-all-facial-recognition-a5e7931e40bc",
        "https://www.scientificamerican.com/article/police-facial-recognition-technology-cant-tell-black-people-apart",
        "https://www.leginfo.legislature.ca.gov/faces/billNavClient.xhtml?bill_id=202320240AB1814",
        "https://leginfo.legislature.ca.gov/faces/billNavClient.xhtml",
        "https://scientificamerican.com/article/police-facial-recognition-technology-cant-tell-black-people-apart/"
    ],
    "external_images": [],
    "entities": {
        "persons": [
            {
                "name": "phil ting",
                "sentiment": "none"
            }
        ],
        "organizations": [
            {
                "name": "eff",
                "sentiment": "none"
            },
            {
                "name": "clearview ai",
                "sentiment": "none"
            }
        ],
        "locations": [
            {
                "name": "san francisco",
                "sentiment": "none"
            },
            {
                "name": "california",
                "sentiment": "none"
            }
        ]
    },
    "rating": null,
    "crawled": "2024-08-09T00:29:09.862+03:00",
    "updated": "2024-08-09T07:55:50.855+03:00"
}