{
    "thread": {
        "uuid": "2a0865d5ec57fadeaf238a4a625008be82fbcb01",
        "url": "https://mobilesyrup.com/2024/02/01/openai-gpt-4-bioweapon-plans",
        "site_full": "mobilesyrup.com",
        "site": "mobilesyrup.com",
        "site_section": "https://ground.news/interest/ai",
        "site_categories": [
            "politics",
            "financial_news",
            "law_government_and_politics",
            "finance"
        ],
        "section_title": "Artificial Intelligence Breaking News Headlines Today | Ground News",
        "title": "OpenAI admits GPT-4 can help make bioweapons, but only a little bit",
        "title_full": "OpenAI admits GPT-4 can help make bioweapons, but only a little bit",
        "published": "2024-02-02T01:59:00.000+02:00",
        "replies_count": 0,
        "participants_count": 1,
        "site_type": "news",
        "country": "CA",
        "main_image": "https://cdn.mobilesyrup.com/wp-content/uploads/2024/02/covid-19-2048x1151-1.jpg",
        "performance_score": 0,
        "domain_rank": 14741,
        "domain_rank_updated": "2024-01-30T12:05:46.000+02:00",
        "reach": null,
        "social": {
            "facebook": {
                "likes": 0,
                "comments": 0,
                "shares": 0
            },
            "gplus": {
                "shares": 0
            },
            "pinterest": {
                "shares": 0
            },
            "linkedin": {
                "shares": 0
            },
            "stumbledupon": {
                "shares": 0
            },
            "vk": {
                "shares": 0
            }
        }
    },
    "uuid": "2a0865d5ec57fadeaf238a4a625008be82fbcb01",
    "url": "https://mobilesyrup.com/2024/02/01/openai-gpt-4-bioweapon-plans",
    "ord_in_thread": 0,
    "parent_url": null,
    "author": "Karandeep Oberoi",
    "published": "2024-02-02T01:59:00.000+02:00",
    "title": "OpenAI admits GPT-4 can help make bioweapons, but only a little bit",
    "text": "OpenAI, the company behind ChatGPT, has [conducted a new study](https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation) that found GPT-4 poses a slight risk when it comes to assisting in creating a bioweapon. The study involved testing the model’s impact on the accuracy and completeness of bioweapon plans and was motivated by a recent executive order from the U.S. government, which [expressed concern](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/) that AI could lower the barriers to entry to produce biological weapons. To evaluate the threat, OpenAI conducted a study with 100 human participants with 50 biology experts with PhDs and professional wet lab experience and 50 student-level participants, with at least one university-level course in biology. The participants were divided into two groups, with one group allowed access to the internet, and the other that was allowed access to the internet and GPT-4. “Each participant was then asked to complete a set of tasks covering aspects of the end-to-end process for biological threat creation,” wrote OpenAI. The results showed that GPT-4 had a small positive effect on the accuracy, completeness, innovation, time taken and self-rated difficulty of the bioweapon plans. The company concluded that “GPT-4 provides at most a mild uplift in biological threat creation accuracy.” It added that “while this uplift is not large enough to be conclusive, our finding is a starting point for continued research and community deliberation.” Since it’s bio weapons we’re talking about, even a “mild uplift” in helping people create such weapons is a serious matter. However, OpenAI did add that bioweapon information is already widely available on the internet, and that its model does not significantly increase risk. You can check out the study here.",
    "highlightText": "",
    "highlightTitle": "",
    "highlightThreadTitle": "",
    "language": "english",
    "sentiment": "negative",
    "categories": [
        "Social Issue",
        "Science and Technology",
        "War, Conflict and Unrest"
    ],
    "external_links": [
        "https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation",
        "https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/",
        "https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence",
        "https://whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/",
        "https://www.openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation"
    ],
    "external_images": [],
    "entities": {
        "persons": [],
        "organizations": [
            {
                "name": "gpt",
                "sentiment": "neutral"
            }
        ],
        "locations": [
            {
                "name": "u.s.",
                "sentiment": "none"
            }
        ]
    },
    "rating": null,
    "crawled": "2024-02-02T02:10:13.165+02:00",
    "updated": "2024-02-02T02:19:35.258+02:00"
}