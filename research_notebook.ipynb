{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d538d4b3",
   "metadata": {},
   "source": [
    "# Aligning ESG Controversy Data with International Guidelines through Semi-Automatic Ontology Construction\n",
    "Here is the notebook used for the research paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f78151c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from openai import AzureOpenAI\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a9879c",
   "metadata": {},
   "source": [
    "### Dataset Creation\n",
    "The data used in this paper comes from the public repository Webz.io https://github.com/Webhose/free-news-datasets\n",
    "\n",
    "Since the data is separated into different categories, we preprocess the repository to have one big file with every single text documents in English, we are also interested in \"Negative\" news article, therefore we filter them too. \n",
    "\n",
    "In order to run this code, you need to clone or download the Webz.io reprository into a `data` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b035391",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = Path(\"data\")\n",
    "# Path to the folder containing the .zip files\n",
    "folder_path = base_folder / \"free-news-datasets-master\" / \"News_Datasets\"\n",
    "# Path to save the final concatenated DataFrame\n",
    "output_path = base_folder / \"research_paper_data.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90df6ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Iterate through all files in the folder and extract relevant zip files\n",
    "for file_name in tqdm(os.listdir(folder_path), desc=\"Extracting zip files\"):\n",
    "    if file_name.endswith(\".zip\") and 'negative' in file_name:\n",
    "        file_path = folder_path / file_name\n",
    "        extract_path = folder_path / file_name[:-4]  # Remove '.zip' from the name\n",
    "        extract_path.mkdir(exist_ok=True)\n",
    "        with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(extract_path)\n",
    "\n",
    "# Step 2: Iterate over all extracted folders and process JSON files\n",
    "webzio_data = pd.DataFrame()\n",
    "\n",
    "iteration_count = 0\n",
    "for root, dirs, files in os.walk(folder_path):\n",
    "    iteration_count += 1\n",
    "\n",
    "for root, dirs, files in tqdm(os.walk(folder_path), desc=\"Processing extracted folders\", total=iteration_count):\n",
    "    for sub_dir in dirs:\n",
    "        sub_dir_path = Path(root) / sub_dir\n",
    "        for file in os.listdir(sub_dir_path):\n",
    "            if file.endswith('.json'):\n",
    "                json_file_path = sub_dir_path / file\n",
    "                try:\n",
    "                    with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "                        file_data = json.load(json_file)\n",
    "                        df = pd.json_normalize(file_data)\n",
    "                        # Filter for English articles only\n",
    "                        df = df[df['language'] == 'english']\n",
    "                        webzio_data = pd.concat([webzio_data, df], ignore_index=True)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {json_file_path}: {e}\")\n",
    "\n",
    "print(f\"Total documents: {len(webzio_data)}\")\n",
    "\n",
    "# Step 3: Save the final concatenated DataFrame to a parquet file\n",
    "webzio_data = webzio_data.drop_duplicates(subset='uuid', keep='first')\n",
    "webzio_data.reset_index(drop=True, inplace=True)\n",
    "webzio_data.to_parquet(output_path, index=False)\n",
    "print(f\"Concatenated data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2b3855",
   "metadata": {},
   "source": [
    "### Dataset Filtering\n",
    "\n",
    "Here we use Azure AI to call our LLM to filter out documents unrelated to ESG (according to RepRisk methodology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "627eb6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt-4o-mini-research\"\n",
    "MODEL_NAME_STR = MODEL_NAME.replace(\"-\", \"_\").replace(\".\", \"_\")\n",
    "CHAT_COMPLETION_PARAMS = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 0,\n",
    "    \"seed\": 42,\n",
    "}\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=os.getenv('AZURE_ENDPOINT'),\n",
    "    api_key=os.getenv('AZURE_API_KEY'),\n",
    "    api_version=os.getenv('AZURE_API_VERSION')\n",
    ")\n",
    "MODEL_OUTPUT_PATH = base_folder / f\"research_paper_data_{MODEL_NAME_STR}.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "b67ccd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19662/19662 [1:39:24<00:00,  3.30it/s]   \n"
     ]
    }
   ],
   "source": [
    "webzio_data['ai_response'] = 0\n",
    "\n",
    "for index, row in tqdm(webzio_data.iterrows(), total=len(webzio_data)):\n",
    "    title = str(row.get('title', '') or '')\n",
    "    text = str(row.get('text', '') or '')\n",
    "    message_text = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": \"\"\"\n",
    "         You are a neutral unbiased expert in world affairs, business and corporate responsibility.\n",
    "         Your task is to determine whether articles are related to ESG (Environment, Social and Governance) incidents. \n",
    "         Rules to follow:\n",
    "         - A company must be mentioned in the article.\n",
    "         - The incident is negative, and the company is being criticized or accused of something.\n",
    "         - The incident must be related to the company's operations, products, or services.\n",
    "         You output either True or False based on the above rules. True if the document is about an ESG incident, False otherwise.\n",
    "         \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        ```\n",
    "        {title}\n",
    "        {text}\n",
    "        ```\n",
    "        \"\"\"}\n",
    "    ]\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            messages=message_text,\n",
    "            **CHAT_COMPLETION_PARAMS\n",
    "        )\n",
    "        response = completion.choices[0].message.content.lower().strip()\n",
    "        webzio_data.loc[index, 'ai_response'] = 1 if 'true' in response else 0\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document: {e}\")\n",
    "        webzio_data.loc[index, 'ai_response'] = 0\n",
    "\n",
    "webzio_data = webzio_data[webzio_data['ai_response'] == 1]\n",
    "webzio_data.to_parquet(MODEL_OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d97f7b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1941 entries, 5 to 19648\n",
      "Data columns (total 59 columns):\n",
      " #   Column                             Non-Null Count  Dtype  \n",
      "---  ------                             --------------  -----  \n",
      " 0   uuid                               1941 non-null   object \n",
      " 1   url                                1941 non-null   object \n",
      " 2   ord_in_thread                      1941 non-null   int64  \n",
      " 3   parent_url                         0 non-null      object \n",
      " 4   author                             1769 non-null   object \n",
      " 5   published                          1941 non-null   object \n",
      " 6   title                              1941 non-null   object \n",
      " 7   text                               1941 non-null   object \n",
      " 8   highlightText                      1941 non-null   object \n",
      " 9   highlightTitle                     1941 non-null   object \n",
      " 10  highlightThreadTitle               1941 non-null   object \n",
      " 11  language                           1941 non-null   object \n",
      " 12  sentiment                          1941 non-null   object \n",
      " 13  categories                         1941 non-null   object \n",
      " 14  external_links                     1941 non-null   object \n",
      " 15  external_images                    1941 non-null   object \n",
      " 16  rating                             0 non-null      object \n",
      " 17  crawled                            1941 non-null   object \n",
      " 18  updated                            1941 non-null   object \n",
      " 19  thread.uuid                        1941 non-null   object \n",
      " 20  thread.url                         1941 non-null   object \n",
      " 21  thread.site_full                   1941 non-null   object \n",
      " 22  thread.site                        1941 non-null   object \n",
      " 23  thread.site_section                1941 non-null   object \n",
      " 24  thread.site_categories             1941 non-null   object \n",
      " 25  thread.section_title               1914 non-null   object \n",
      " 26  thread.title                       1941 non-null   object \n",
      " 27  thread.title_full                  1941 non-null   object \n",
      " 28  thread.published                   1941 non-null   object \n",
      " 29  thread.replies_count               1941 non-null   int64  \n",
      " 30  thread.participants_count          1941 non-null   int64  \n",
      " 31  thread.site_type                   1941 non-null   object \n",
      " 32  thread.country                     1936 non-null   object \n",
      " 33  thread.main_image                  1941 non-null   object \n",
      " 34  thread.performance_score           1941 non-null   int64  \n",
      " 35  thread.domain_rank                 1941 non-null   int64  \n",
      " 36  thread.domain_rank_updated         1941 non-null   object \n",
      " 37  thread.reach                       0 non-null      object \n",
      " 38  thread.social.facebook.likes       1941 non-null   int64  \n",
      " 39  thread.social.facebook.comments    1941 non-null   int64  \n",
      " 40  thread.social.facebook.shares      1941 non-null   int64  \n",
      " 41  thread.social.gplus.shares         1026 non-null   float64\n",
      " 42  thread.social.pinterest.shares     1026 non-null   float64\n",
      " 43  thread.social.linkedin.shares      1026 non-null   float64\n",
      " 44  thread.social.stumbledupon.shares  1026 non-null   float64\n",
      " 45  thread.social.vk.shares            1941 non-null   int64  \n",
      " 46  entities.persons                   1941 non-null   object \n",
      " 47  entities.organizations             1941 non-null   object \n",
      " 48  entities.locations                 1941 non-null   object \n",
      " 49  ai_allow                           982 non-null    object \n",
      " 50  webz_reporter                      1312 non-null   object \n",
      " 51  syndication.syndicated             836 non-null    object \n",
      " 52  syndication.syndicate_id           83 non-null     object \n",
      " 53  syndication.first_syndicated       547 non-null    object \n",
      " 54  topics                             129 non-null    object \n",
      " 55  has_canonical                      132 non-null    object \n",
      " 56  canonical                          401 non-null    object \n",
      " 57  syndication                        0 non-null      object \n",
      " 58  ai_response                        1941 non-null   int64  \n",
      "dtypes: float64(4), int64(10), object(45)\n",
      "memory usage: 909.8+ KB\n"
     ]
    }
   ],
   "source": [
    "webzio_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd90eff",
   "metadata": {},
   "source": [
    "### Entity Extraction\n",
    "\n",
    "As a first step in our data we extract the entities, as this data could benefit for the pattern extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b78cba24",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_extraction_prompt = \"\"\"\n",
    "You are a neutral and unbiased expert in world affairs, business, and corporate responsibility, with specialized knowledge\n",
    "in Named Entity Recognition (NER).\n",
    "Your task is to extract and categorize all named entities from a given news article while ensuring accuracy and relevance.\n",
    "\n",
    "Entity Extraction Guidelines:\n",
    "Focus on extracting real and named entities only. Ignore generic terms like \"a person\" or \"a company.\"\n",
    "Categorize entities into the following types:\n",
    "- PER (Persons): Individuals mentioned in the article.\n",
    "- ORG (Organizations & Companies): Businesses, institutions, or recognized groups.\n",
    "- LOC (Locations): Countries, cities, and geographical places.\n",
    "\n",
    "If an entity appears in different forms (acronyms, full names, alternative spellings, etc.), group them under a single list.\n",
    "Example: \"United States\" and \"US\" → [\"United States\", \"US\"]\n",
    "Example: \"X\" and \"X.com\" → [\"X\", \"X.com\"]\n",
    "\n",
    "Additional Considerations:\n",
    "Since the input is a news article, explicitly ignore metadata or unrelated elements such as:\n",
    "- Author names\n",
    "- News categories, timestamps, and URLs\n",
    "- Website-related text (e.g., cookie pop-ups)\n",
    "\n",
    "Example Input & Output:\n",
    "\n",
    "# Article:\n",
    "Date: 01/01/2024\n",
    "Apple Inc. is a company that makes computers. Its most famous one is the MacBook Pro.\n",
    "Apple's HQ is in Los Angeles; however, the company has been criticized by Amazon for using forced labor in China.\n",
    "Jeff Bezos, the CEO of Amazon, has been vocal about this issue. MacBooks have been found in Amazon's warehouses.\n",
    "Meanwhile, Tesla has announced its latest Model S Plaid, which competes with Porsche Taycan.\n",
    "\n",
    "# Expected Output \n",
    "{\n",
    "    \"ORG\": [[\"Apple Inc.\", \"Apple\"], [\"Amazon\"], [\"Tesla\"], [\"Porsche\"]],\n",
    "    \"PER\": [[\"Jeff Bezos\"]],\n",
    "    \"LOC\": [[\"Los Angeles\"], [\"China\"]]\n",
    "}\n",
    "\n",
    "# Article\n",
    "2023-12-28 21:15:49\n",
    "Published 28. December 2023, 10:15 p.m\n",
    "Seattle, USA: Boeing in trouble again because of a loose screw on a 737 MAX\n",
    "Boeing is in trouble again with its 737 MAX: After an airline discovered a missing nut, all aircraft of the type are now to be inspected.\n",
    "1 / 4\n",
    "An airline discovered a missing nut on a bolt. A loose screw was also discovered on a 737 MAX that had not yet been delivered.\n",
    "imago images/ZUMA Wire\n",
    "Now all machines worldwide are to be checked.\n",
    "REUTERS\n",
    "The problem concerns a tie rod used to control rudder movements.\n",
    "REUTERS\n",
    "That's what it's about\n",
    "-\n",
    "An airline discovered a missing nut on a control rod on a Boeing 737 MAX.\n",
    "-\n",
    "The US aviation safety authority has now asked all airlines to check the relevant component.\n",
    "-\n",
    "The aircraft manufacturer's shares fell after the news.\n",
    "The US aircraft manufacturer Boeing cannot relax - this time it is about a bolt in the rudder control system of the 737 MAX model.\n",
    "The Federal Aviation Administration (FAA) urged operators of newer single-aisle aircraft in a statement Thursday to inspect certain control rods used to control\n",
    "rudder movements for loss of parts. The agency called in a statement to “closely monitor targeted inspections of Boeing 737 MAX aircraft for the possibility of a\n",
    "loose screw in the rudder control system.”\n",
    "List of Boeing shows which airlines rely on the type.\n",
    "Have you been following on Whatsapp for 20 minutes?\n",
    "Stay informed and subscribe to the 20 Minutes WhatsApp channel: Then you will receive an update with our most moving stories directly to your cell phone in the\n",
    "morning and evening - hand-picked, informative and inspiring.\n",
    "(DPA/trx)\n",
    "1703804980\n",
    "#Boeing #trouble #due #missing #nut #MAX\n",
    "\n",
    "# Output\n",
    "{\n",
    "    \"ORG\": [[\"Boeing\"], [\"Federal Aviation Administration\", \"FAA\"]],\n",
    "    \"PER\": [],\n",
    "    \"LOC\": [[\"Seattle\"], [\"USA\", \"US\"]]\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "9d3ab129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1941/1941 [30:30<00:00,  1.06it/s] \n"
     ]
    }
   ],
   "source": [
    "webzio_data['entity_response'] = ''\n",
    "for index, row in tqdm(webzio_data.iterrows(), total=len(webzio_data)):\n",
    "    entity_resp = row['entity_response']\n",
    "    if isinstance(entity_resp, str) and entity_resp.strip() != '' and entity_resp != 'ERROR':\n",
    "        continue\n",
    "\n",
    "    message_text = [\n",
    "        {\"role\": \"system\", \"content\": entity_extraction_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        # Article\n",
    "        {row['title'] if row['title'] else ''}\n",
    "        {row['text'] if row['text'] else ''}\n",
    "\n",
    "        # Output\n",
    "        \"\"\"}\n",
    "    ]\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            messages=message_text,\n",
    "            **CHAT_COMPLETION_PARAMS\n",
    "        )\n",
    "        webzio_data.at[index, 'entity_response'] = completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document: {e}\")\n",
    "        webzio_data.at[index, 'entity_response'] = 'ERROR'\n",
    "\n",
    "def clean_json_text(json_text):\n",
    "    if not isinstance(json_text, str):\n",
    "        return {}\n",
    "    try:\n",
    "        for tag in ('```json', '```'):\n",
    "            json_text = json_text.replace(tag, '')\n",
    "    except Exception as e:\n",
    "        print(f'Error cleaning json: {e}')\n",
    "        print(json_text)\n",
    "    try:\n",
    "        return json.loads(json_text)\n",
    "    except Exception as e:\n",
    "        print(f'Error loading json: {e}')\n",
    "        print(json_text)\n",
    "        return {}\n",
    "\n",
    "webzio_data['json_entities'] = webzio_data['entity_response'].apply(clean_json_text)\n",
    "webzio_data['json_entities'] = webzio_data['json_entities'].apply(json.dumps)\n",
    "webzio_data.to_parquet(MODEL_OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c260d7",
   "metadata": {},
   "source": [
    "### Regulation Summarization\n",
    "\n",
    "Here we pick as our regulation the UNGC Principles.\n",
    "The text in for the UNGC principle has been manually extracted from https://unglobalcompact.org/what-is-gc/mission/principles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "bdcbbe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "regulation_name = \"UN Global Compact (UNGC) Principle\"\n",
    "regulation_dir = \"ungc_principles\"\n",
    "\n",
    "regulation_summary_prompt = \"\"\"\n",
    "You are an expert on {regulation_name} with comprehensive knowledge of their content, context, and application.\n",
    "\n",
    "Task: Summarize the following {regulation_name} while maintaining all substantive requirements and actionable guidance.\n",
    "This summary will serve as the foundation for creating pattern recognition algorithms to identify principles in news articles.\n",
    "\n",
    "Summary Guidelines:\n",
    "- Create a concise summary (approximately 300-500 words)\n",
    "- Maintain all key requirements, obligations, and implementation guidance\n",
    "- Preserve specific criteria that businesses must satisfy\n",
    "- Include relevant standards, frameworks, or measurement approaches mentioned\n",
    "- Omit purely historical context, duplicative statements, or non-essential background\n",
    "\n",
    "Structure:\n",
    "1. Begin with a 1-2 sentence overview of the {regulation_name}'s core focus\n",
    "2. Organize content using clear markdown headings and subheadings\n",
    "3. Use bullet points for listing specific requirements or actions\n",
    "4. Bold key terms or thresholds that would be essential for pattern matching\n",
    "\n",
    "The {regulation_name} to summarize is:\n",
    "{sub_regulation}\n",
    "\n",
    "Output only the formatted summary without introduction, explanation, or commentary.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "d8ae9cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing principle-8.txt...\n",
      "Processing principle-5.txt...\n",
      "Processing principle-7.txt...\n",
      "Processing principle-3.txt...\n",
      "Processing principle-6.txt...\n",
      "Processing principle-10.txt...\n",
      "Processing principle-4.txt...\n",
      "Processing principle-1.txt...\n",
      "Processing principle-2.txt...\n",
      "Processing principle-9.txt...\n"
     ]
    }
   ],
   "source": [
    "for filename in os.listdir(regulation_dir):\n",
    "    if filename.endswith('.txt') and not filename.endswith('summary.txt') and filename.rstrip('.txt').split('-')[-1].isdigit():\n",
    "        with open(os.path.join(regulation_dir, filename), 'r') as f:\n",
    "            ungc_principle = f.read()\n",
    "        print(f\"Processing {filename}...\")\n",
    "        completion = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"system\",\n",
    "                       \"content\": regulation_summary_prompt.format(regulation_name=regulation_name, sub_regulation=ungc_principle)}],\n",
    "            **CHAT_COMPLETION_PARAMS\n",
    "            )\n",
    "        with open(os.path.join(f'{regulation_dir}/{filename[:-4]}-{MODEL_NAME_STR}-summary.txt'), 'w') as f:\n",
    "            f.write(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495734a2",
   "metadata": {},
   "source": [
    "### Create Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "421e7909",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_PATTERNS = 1\n",
    "NUMBER_OF_REGULATIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "e58fbb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_prompt = \"\"\"\n",
    "You are a neutral and unbiased expert in business ethics, corporate responsibility, and international standards,\n",
    "with specialized knowledge in {regulation_name} and their practical application.\n",
    "\n",
    "Your task is to develop an extraction framework that can systematically identify potential {regulation_name} violations in news articles.\n",
    "This framework will:\n",
    "1. Define precise extraction patterns calibrated to each {regulation_name}\n",
    "2. Extract structured information from news text using these patterns\n",
    "3. Match extracted information to specific {regulation_name} and categorize potential violations\n",
    "\n",
    "## Pattern Requirements:\n",
    "- Each pattern must be formalized as a relation triple (Entity A, Action, Entity B) where:\n",
    "  * Entity A is typically the business/corporation or its representatives\n",
    "  * Action is the specific behavior, decision, or practice that may violate {regulation_name}\n",
    "  * Entity B is the affected party, resource, or context\n",
    "\n",
    "- Patterns must capture both:\n",
    "  * Direct violations (explicit actions that contradict principles)\n",
    "  * Indirect violations (failures to implement required safeguards or due diligence)\n",
    "\n",
    "- Patterns should account for varied linguistic expressions of the same underlying violation\n",
    "\n",
    "## {regulation_name}:\n",
    "{concatenated_regulation}\n",
    "\n",
    "## Examples of Effective Extraction Patterns:\n",
    "\n",
    "1. Human Rights Violation Pattern: (Company, exploits/endangers/violates, rights of community/workers/individuals)\n",
    "2. Environmental Harm Pattern: (Company, pollutes/damages/depletes, ecosystem/resource/environment)\n",
    "3. Labor Abuse Pattern: (Company, restricts/prevents/penalizes, worker organization/collective action)\n",
    "4. Corruption Pattern: (Company, bribes/pays/transfers value to, government official/regulator/decision-maker)\n",
    "\n",
    "## Your Deliverables:\n",
    "1. Create {number_of_patterns} specialized extraction patterns for EACH {regulation_name} ({total_number_of_patterns} total patterns)\n",
    "2. For each pattern, provide:\n",
    "   - Formal triple structure (A, B, C)\n",
    "   - Add an fictitious example of a sentence that could be extracted with said pattern (in its triple structure) in a field called \"To look for\"\n",
    "   - Add also a counter example of a sentence that should not be extracted with said pattern (in its triple structure) in a field called \"To ignore\"\n",
    "3. Number the patterns sequentially from 1 to 30 (Do not number them like 1.x, 2.x, etc. Just use 1, 2, 3, ..., {total_number_of_patterns})\n",
    "\"\"\"\n",
    "\n",
    "parameters = {\n",
    "    \"concatenated_regulation\": '',\n",
    "    \"regulation_name\": regulation_name,\n",
    "    \"number_of_patterns\": NUMBER_OF_PATTERNS,\n",
    "    \"total_number_of_patterns\": NUMBER_OF_PATTERNS * NUMBER_OF_REGULATIONS,\n",
    "}\n",
    "def extract_principle_number(filename):\n",
    "    match = re.search(r'(\\d+)-{}-summary.txt$'.format(MODEL_NAME_STR), filename)\n",
    "    return int(match.group(1)) if match else float('inf')\n",
    "\n",
    "all_sub_regulations = []\n",
    "\n",
    "# Get all summary filenames and sort them by principle number\n",
    "summary_filenames = [f for f in os.listdir(regulation_dir) if f.endswith(f'{MODEL_NAME_STR}-summary.txt')]\n",
    "summary_filenames_sorted = sorted(summary_filenames, key=extract_principle_number)\n",
    "\n",
    "\n",
    "for filename in summary_filenames_sorted:\n",
    "    with open(os.path.join(regulation_dir, filename), 'r') as f:\n",
    "        sub_regulation = f.read()\n",
    "    all_sub_regulations.append(sub_regulation)\n",
    "\n",
    "parameters[\"concatenated_regulation\"] = \"\\n\\n\".join(all_sub_regulations)\n",
    "\n",
    "message_text = [\n",
    "    {\"role\": \"system\",\n",
    "        \"content\": f\"\"\"\n",
    "        {pattern_prompt.format(**parameters)}\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "completion = client.chat.completions.create(\n",
    "    messages=message_text,\n",
    "   **CHAT_COMPLETION_PARAMS\n",
    "   )\n",
    "if not os.path.exists('patterns'):\n",
    "    os.makedirs('patterns')\n",
    "with open(f'patterns/extraction_patterns_{MODEL_NAME_STR}_{NUMBER_OF_PATTERNS}.txt', 'w') as f:\n",
    "    f.write(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba542b5",
   "metadata": {},
   "source": [
    "### Predict Generated Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "8eabc1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_regulation_patterns = \"\"\"\n",
    "# Task: Extract {regulation_name} Violations from News Articles\n",
    "\n",
    "You are a strict pattern matcher. \n",
    "You must only match text if it exactly or strongly resembles one of the known patterns.\n",
    "Do not infer or assume meaning that is not explicitly present in the text.\n",
    "\n",
    "Extract patterns from news articles that indicate potential violations of {regulation_name}. \n",
    "The goal is to identify as many valid patterns as possible that show how businesses may be violating these important international standards.\n",
    "\n",
    "## Important Instructions:\n",
    "- Extract information that matches one of the predefined patterns\n",
    "- If a company or entity name is missing but clearly implied, include it\n",
    "- Output only in the format specified below\n",
    "- Keep extracted patterns concise without losing critical meaning\n",
    "- If no patterns match, return an empty `\"patterns\"` array\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "expected_format = \"\"\"\n",
    "## Expected JSON Output Format\n",
    "{\n",
    "    \"patterns\": [\n",
    "        {\n",
    "            \"pattern1\": [[\"Company A\", \"violates\", \"human rights of indigenous communities\"]]\n",
    "        },\n",
    "        {\n",
    "            \"pattern20\": [[\"Mining Corporation\", \"engages in\", \"environmentally harmful practices\"]]\n",
    "        },\n",
    "        {\n",
    "            \"pattern28\": [\n",
    "                [\"Telecom Company\", \"engages in\", \"bribery of local officials\"],\n",
    "                [\"Same Company\", \"engages in\", \"bribery of regulatory officials\"]\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "## Final Notes:\n",
    "- If multiple instances of a pattern exist, include all in the respective array\n",
    "- If a pattern isn't found in the text, don't include it in the output\n",
    "- Ensure correct structure with no trailing commas, only use double quotes\n",
    "- Prioritize accuracy over quantity of extractions\n",
    "- You must only match text if it exactly or strongly resembles one of the known patterns.\n",
    "- Do not infer or assume meaning that is not explicitly present in the text.\n",
    "- Do not output anything except the matched patterns in the specified format.\n",
    "\n",
    "# Follow these 3 steps : \n",
    "1. Extract potential triples from the text.\n",
    "2. Match those triples to predefined patterns.\n",
    "3. Check the triples are exact or strong matches. \n",
    "4. Output only the matched patterns in the specified format.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "caac686f",
   "metadata": {},
   "outputs": [],
   "source": [
    "webzio_data = pd.read_parquet(MODEL_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "c0a8f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "webzio_data['pattern_response'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "0d690699",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1941/1941 [20:42<00:00,  1.56it/s] \n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('patterns', f'extraction_patterns_{MODEL_NAME_STR}_{NUMBER_OF_PATTERNS}.txt'), 'r') as f:\n",
    "    generated_patterns = f.read()\n",
    "\n",
    "for index, row in tqdm(webzio_data.iterrows(), total=len(webzio_data)):\n",
    "    if row['pattern_response'] != '' and row['pattern_response'] != 'ERROR' and type(row['pattern_response']) is str:\n",
    "        continue\n",
    "    message_text = [\n",
    "        {\"role\": \"system\",\n",
    "         \"content\": query_regulation_patterns.format(regulation_name=regulation_name) + generated_patterns + expected_format},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "        # Article\n",
    "        {row['title'] if row['title'] else ''}\n",
    "        {row['text'] if row['text'] else ''}\n",
    "\n",
    "        # Entities\n",
    "        {row['json_entities'] if row['json_entities'] else ''}\n",
    "        \n",
    "        # Output\n",
    "        \"\"\"}\n",
    "    ]\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            messages=message_text,\n",
    "            **CHAT_COMPLETION_PARAMS\n",
    "            )\n",
    "        webzio_data.at[index, 'pattern_response'] = completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document: {e}\")\n",
    "        webzio_data.at[index, 'pattern_response'] = 'ERROR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "874a140b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading json: Expecting property name enclosed in double quotes: line 8 column 13 (char 229)\n",
      "{\n",
      "    \"patterns\": [\n",
      "        {\n",
      "            \"pattern7\": [[\"Apple\", \"neglects\", \"environmental precautions\"]]\n",
      "        },\n",
      "        {\n",
      "            \"pattern8\": [[\"Apple\", \"fails to promote\", \"environmental responsibility\"]],\n",
      "            [\"Samsung\", \"fails to promote\", \"environmental responsibility\"],\n",
      "            [\"Google\", \"fails to promote\", \"environmental responsibility\"]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Error loading json: Expecting property name enclosed in double quotes: line 8 column 13 (char 229)\n",
      "{\n",
      "    \"patterns\": [\n",
      "        {\n",
      "            \"pattern7\": [[\"Apple\", \"neglects\", \"environmental precautions\"]]\n",
      "        },\n",
      "        {\n",
      "            \"pattern8\": [[\"Apple\", \"fails to promote\", \"environmental responsibility\"]],\n",
      "            [\"Samsung\", \"fails to promote\", \"environmental responsibility\"],\n",
      "            [\"Google\", \"fails to promote\", \"environmental responsibility\"]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Error loading json: Expecting property name enclosed in double quotes: line 11 column 13 (char 349)\n",
      "{\n",
      "    \"patterns\": [\n",
      "        {\n",
      "            \"pattern1\": [[\"NVPERS\", \"infringes on\", \"human rights of individuals\"]]\n",
      "        },\n",
      "        {\n",
      "            \"pattern3\": [[\"AllianceBernstein\", \"restricts\", \"workers' right to organize\"]]\n",
      "        },\n",
      "        {\n",
      "            \"pattern7\": [[\"AllianceBernstein\", \"neglects\", \"environmental precautions\"]],\n",
      "            [\"SSGA\", \"neglects\", \"environmental precautions\"]\n",
      "        },\n",
      "        {\n",
      "            \"pattern8\": [[\"AllianceBernstein\", \"fails to promote\", \"environmental responsibility\"]],\n",
      "            [\"SSGA\", \"fails to promote\", \"environmental responsibility\"]\n",
      "        },\n",
      "        {\n",
      "            \"pattern10\": [[\"BlackRock\", \"engages in\", \"bribery\"]]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Error loading json: Expecting property name enclosed in double quotes: line 11 column 13 (char 349)\n",
      "{\n",
      "    \"patterns\": [\n",
      "        {\n",
      "            \"pattern1\": [[\"NVPERS\", \"infringes on\", \"human rights of individuals\"]]\n",
      "        },\n",
      "        {\n",
      "            \"pattern3\": [[\"AllianceBernstein\", \"restricts\", \"workers' right to organize\"]]\n",
      "        },\n",
      "        {\n",
      "            \"pattern7\": [[\"AllianceBernstein\", \"neglects\", \"environmental precautions\"]],\n",
      "            [\"Mellon Capital\", \"neglects\", \"environmental precautions\"],\n",
      "            [\"SSGA\", \"neglects\", \"environmental precautions\"]\n",
      "        },\n",
      "        {\n",
      "            \"pattern8\": [[\"AllianceBernstein\", \"fails to promote\", \"environmental responsibility\"]],\n",
      "            [\"Mellon Capital\", \"fails to promote\", \"environmental responsibility\"],\n",
      "            [\"SSGA\", \"fails to promote\", \"environmental responsibility\"]\n",
      "        },\n",
      "        {\n",
      "            \"pattern10\": [[\"AllianceBernstein\", \"engages in\", \"bribery\"]],\n",
      "            [\"BlackRock\", \"engages in\", \"bribery\"]\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Error loading json: Expecting property name enclosed in double quotes: line 8 column 13 (char 239)\n",
      "{\n",
      "    \"patterns\": [\n",
      "        {\n",
      "            \"pattern7\": [[\"Paris 2024\", \"neglects\", \"environmental precautions\"]]\n",
      "        },\n",
      "        {\n",
      "            \"pattern8\": [[\"Paris 2024\", \"fails to promote\", \"environmental responsibility\"]],\n",
      "            [\"Toyota\", \"fails to promote\", \"environmental responsibility\"]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "webzio_data['ungc_patterns'] = webzio_data['pattern_response'].apply(clean_json_text)\n",
    "webzio_data['ungc_patterns'] = webzio_data['ungc_patterns'].apply(json.dumps)\n",
    "webzio_data.to_parquet(base_folder / f\"research_paper_data_{MODEL_NAME_STR}_{NUMBER_OF_PATTERNS}.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bcf566",
   "metadata": {},
   "source": [
    "### Double checking predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e837f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "double_check_prompt = \"\"\"\n",
    "You are given a text and a list of patterns. Each pattern describes a type of relationship or action involving specific entities.\n",
    "Your task is to examine the text and return a list of triples. \n",
    "The triple should consist of a pattern ID, one sentence from the text that correspond to the pattern, and an explanation why the sentence supports the pattern.\n",
    "\n",
    "Only include a pattern if the content of the text clearly supports or describes it. Do not infer beyond what is explicitly or strongly implied.\n",
    "If you are not sure about a pattern, do not include it.\n",
    "If the text does not support the pattern and instead contradicts it, do not include it.\n",
    "\n",
    "If there are multiple times the same pattern, only include the first one.\n",
    "\n",
    "### TEXT:\n",
    "{article_text}\n",
    "\n",
    "### PATTERNS:\n",
    "{patterns}\n",
    "\n",
    "### OUTPUT FORMAT:\n",
    "[(\"pattern1\", <sentence in the text>, <explanation why the sentence supports the pattern>), (\"pattern4\", ...)]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe616f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1491/1491 [11:47<00:00,  2.11it/s] \n"
     ]
    }
   ],
   "source": [
    "webzio_data['double_check_response'] = ''\n",
    "\n",
    "for index, row in tqdm(webzio_data.iterrows(), total=len(webzio_data)):\n",
    "    if row['double_check_response'] != '' and row['double_check_response'] != 'ERROR' and type(row['double_check_response']) is str:\n",
    "        continue\n",
    "    message_text = [\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": double_check_prompt.format(\n",
    "            article_text=row['text'] if row['text'] else '',\n",
    "            patterns=row['ungc_patterns'] if row['ungc_patterns'] else '')},\n",
    "    ]\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            messages=message_text,\n",
    "            **CHAT_COMPLETION_PARAMS\n",
    "            )\n",
    "        webzio_data.at[index, 'double_check_response'] = completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document: {e}\")\n",
    "        webzio_data.at[index, 'double_check_response'] = 'ERROR'\n",
    "\n",
    "webzio_data.to_parquet(MODEL_OUTPUT_PATH, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541ff4e7",
   "metadata": {},
   "source": [
    "# Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c0401305",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = pd.read_parquet(base_folder / f\"research_paper_data_{MODEL_NAME_STR}_{NUMBER_OF_PATTERNS}.parquet\")\n",
    "analyzed_data = pd.read_csv('./labeled_data/ungc_label_200.csv')\n",
    "merged_data = pd.merge(analyzed_data, prediction_data, on='uuid', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d664f04e",
   "metadata": {},
   "source": [
    "## Predicted Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "37b64562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_predicted_unchecked_labels_to_ungc_principles(label_list):\n",
    "    ungc_labels = set()\n",
    "    try:\n",
    "        if isinstance(label_list, float):\n",
    "            return []\n",
    "        for label in ast.literal_eval(label_list)['patterns']:\n",
    "            pattern_id = list(label.keys())[0].strip('pattern')\n",
    "            ungc_labels.add(math.ceil(int(pattern_id) / NUMBER_OF_PATTERNS))\n",
    "    except (ValueError, SyntaxError):\n",
    "        numbers = re.findall(r'pattern(\\d+)', label_list)\n",
    "        for num in numbers:\n",
    "            ungc_labels.add(math.ceil(int(num) / NUMBER_OF_PATTERNS))\n",
    "    return list(ungc_labels)\n",
    "\n",
    "merged_data['predicted_labels'] = merged_data['pattern_response'].apply(convert_predicted_unchecked_labels_to_ungc_principles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "377924ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_human_label_to_list(human_label):\n",
    "    human_label_set = []\n",
    "    for el in human_label.strip('[').strip(']').split(' '):\n",
    "        if el.strip() != '':\n",
    "            human_label_set.append(int(el.strip()))\n",
    "    return human_label_set\n",
    "\n",
    "merged_data['labels'] = merged_data['human_label'].apply(convert_human_label_to_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a937dc2",
   "metadata": {},
   "source": [
    "## Double Checked Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ed15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import ast\n",
    "import re\n",
    "\n",
    "def convert_predicted_labels_to_ungc_principles(label_list):\n",
    "    ungc_labels = set()\n",
    "    if not isinstance(label_list, str) or label_list.strip() == '':\n",
    "        return []\n",
    "    try:\n",
    "        for label in ast.literal_eval(label_list):\n",
    "            pattern_id = label[0].strip('pattern')\n",
    "            ungc_labels.add(math.ceil(int(pattern_id) / NUMBER_OF_PATTERNS))\n",
    "    except (ValueError, SyntaxError):\n",
    "        numbers = re.findall(r'pattern(\\d+)', label_list)\n",
    "        for num in numbers:\n",
    "            ungc_labels.add(math.ceil(int(num) / NUMBER_OF_PATTERNS))\n",
    "    return list(ungc_labels)\n",
    "\n",
    "merged_data['predicted_checked_labels'] = merged_data['double_check_response'].apply(convert_predicted_labels_to_ungc_principles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e3f99f",
   "metadata": {},
   "source": [
    "## Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "51ad314c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 & 39 & 27 & 28 & 106 & 0.58 & 0.72 & 0.59 & 0.59 \\\\\n",
      "2 & 21 & 5 & 18 & 156 & 0.54 & 0.89 & 0.81 & 0.65 \\\\\n",
      "3 & 16 & 2 & 69 & 113 & 0.19 & 0.65 & 0.89 & 0.31 \\\\\n",
      "4 & 7 & 7 & 38 & 148 & 0.16 & 0.78 & 0.50 & 0.24 \\\\\n",
      "5 & 5 & 2 & 31 & 162 & 0.14 & 0.83 & 0.71 & 0.23 \\\\\n",
      "6 & 9 & 0 & 48 & 143 & 0.16 & 0.76 & 1.00 & 0.27 \\\\\n",
      "7 & 17 & 4 & 52 & 127 & 0.25 & 0.72 & 0.81 & 0.38 \\\\\n",
      "8 & 15 & 3 & 35 & 147 & 0.30 & 0.81 & 0.83 & 0.44 \\\\\n",
      "9 & 0 & 1 & 0 & 199 & 0.00 & 0.99 & 0.00 & nan \\\\\n",
      "10 & 11 & 5 & 40 & 144 & 0.22 & 0.78 & 0.69 & 0.33 \\\\\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "y_true_bin = mlb.fit_transform(merged_data['labels'])\n",
    "y_pred_bin = mlb.transform(merged_data['predicted_labels'])\n",
    "\n",
    "labels = mlb.classes_\n",
    "conf_matrices = multilabel_confusion_matrix(y_true_bin, y_pred_bin)\n",
    "\n",
    "# Start LaTeX output\n",
    "latex_lines = []\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    tn, fp, fn, tp = conf_matrices[i].ravel()\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall    = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1        = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else float('nan')\n",
    "    accuracy  = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "    line = f\"{label} & {tp} & {fn} & {fp} & {tn} & {precision:.2f} & {accuracy:.2f} & {recall:.2f} & {f1:.2f} \\\\\\\\\"\n",
    "    latex_lines.append(line)\n",
    "\n",
    "# Join and print LaTeX\n",
    "latex_output = \"\\n\".join(latex_lines)\n",
    "print(latex_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2418e406",
   "metadata": {},
   "source": [
    "For a more visually appealing table you can copy-paste the numbers below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "099d226b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\n",
       "$$\n",
       "\\begin{array}{lcccccccc}\n",
       "\\textbf{Principle} & \\textbf{TP} & \\textbf{FN} & \\textbf{FP} & \\textbf{TN} & \\textbf{Precision} & \\textbf{Accuracy} & \\textbf{Recall} & \\textbf{F1} \\\\\n",
       "\\hline\n",
       "1 & 39 & 27 & 28 & 106 & 0.58 & 0.72 & 0.59 & 0.59 \\\\\n",
       "2 & 21 & 5 & 18 & 156 & 0.54 & 0.89 & 0.81 & 0.65 \\\\\n",
       "3 & 16 & 2 & 69 & 113 & 0.19 & 0.65 & 0.89 & 0.31 \\\\\n",
       "4 & 7 & 7 & 38 & 148 & 0.16 & 0.78 & 0.50 & 0.24 \\\\\n",
       "5 & 5 & 2 & 31 & 162 & 0.14 & 0.83 & 0.71 & 0.23 \\\\\n",
       "6 & 9 & 0 & 48 & 143 & 0.16 & 0.76 & 1.00 & 0.27 \\\\\n",
       "7 & 17 & 4 & 52 & 127 & 0.25 & 0.72 & 0.81 & 0.38 \\\\\n",
       "8 & 15 & 3 & 35 & 147 & 0.30 & 0.81 & 0.83 & 0.44 \\\\\n",
       "9 & 0 & 1 & 0 & 199 & 0.00 & 0.99 & 0.00 & nan \\\\\n",
       "10 & 11 & 5 & 40 & 144 & 0.22 & 0.78 & 0.69 & 0.33 \\\\\n",
       "\\end{array}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Latex\n",
    "\n",
    "latex_code = r\"\"\"\n",
    "$$\n",
    "\\begin{array}{lcccccccc}\n",
    "\\textbf{Principle} & \\textbf{TP} & \\textbf{FN} & \\textbf{FP} & \\textbf{TN} & \\textbf{Precision} & \\textbf{Accuracy} & \\textbf{Recall} & \\textbf{F1} \\\\\n",
    "\\hline\n",
    "1 & 39 & 27 & 28 & 106 & 0.58 & 0.72 & 0.59 & 0.59 \\\\\n",
    "2 & 21 & 5 & 18 & 156 & 0.54 & 0.89 & 0.81 & 0.65 \\\\\n",
    "3 & 16 & 2 & 69 & 113 & 0.19 & 0.65 & 0.89 & 0.31 \\\\\n",
    "4 & 7 & 7 & 38 & 148 & 0.16 & 0.78 & 0.50 & 0.24 \\\\\n",
    "5 & 5 & 2 & 31 & 162 & 0.14 & 0.83 & 0.71 & 0.23 \\\\\n",
    "6 & 9 & 0 & 48 & 143 & 0.16 & 0.76 & 1.00 & 0.27 \\\\\n",
    "7 & 17 & 4 & 52 & 127 & 0.25 & 0.72 & 0.81 & 0.38 \\\\\n",
    "8 & 15 & 3 & 35 & 147 & 0.30 & 0.81 & 0.83 & 0.44 \\\\\n",
    "9 & 0 & 1 & 0 & 199 & 0.00 & 0.99 & 0.00 & nan \\\\\n",
    "10 & 11 & 5 & 40 & 144 & 0.22 & 0.78 & 0.69 & 0.33 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\"\"\"\n",
    "display(Latex(latex_code))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Genearal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
